{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Concurrency & Parallelism\n",
        "\n",
        "This lesson covers concurrent and parallel programming concepts in Python.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this lesson, you will be able to:\n",
        "\n",
        "1. **Concurrency vs Parallelism**\n",
        "   - Understand the difference between concurrency and parallelism\n",
        "   - Know when to use each approach\n",
        "   - Understand Python's Global Interpreter Lock (GIL)\n",
        "\n",
        "2. **Threading**\n",
        "   - Use threading for I/O-bound tasks\n",
        "   - Handle shared data safely\n",
        "   - Use ThreadPoolExecutor for thread management\n",
        "\n",
        "3. **Multiprocessing**\n",
        "   - Use multiprocessing for CPU-bound tasks\n",
        "   - Handle inter-process communication\n",
        "   - Use ProcessPoolExecutor for process management\n",
        "\n",
        "4. **Asyncio**\n",
        "   - Use asyncio for asynchronous programming\n",
        "   - Handle coroutines and tasks\n",
        "   - Implement async/await patterns\n",
        "\n",
        "5. **Synchronization**\n",
        "   - Use locks, semaphores, and other synchronization primitives\n",
        "   - Handle race conditions\n",
        "   - Implement thread-safe data structures\n",
        "\n",
        "6. **Performance Optimization**\n",
        "   - Choose the right concurrency model\n",
        "   - Optimize for different types of workloads\n",
        "   - Measure and improve performance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Concurrency vs Parallelism\n",
        "\n",
        "Understanding the difference between concurrency and parallelism is crucial for choosing the right approach.\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "- **Concurrency**: Multiple tasks making progress but not necessarily simultaneously\n",
        "- **Parallelism**: Multiple tasks executing simultaneously\n",
        "- **GIL**: Global Interpreter Lock that prevents true parallelism in Python threads\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Concurrency vs Parallelism\n",
        "print(\"1. Concurrency vs Parallelism\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "import time\n",
        "import threading\n",
        "import multiprocessing\n",
        "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
        "\n",
        "def cpu_bound_task(n):\n",
        "    \"\"\"CPU-bound task that performs calculations.\"\"\"\n",
        "    result = 0\n",
        "    for i in range(n):\n",
        "        result += i ** 2\n",
        "    return result\n",
        "\n",
        "def io_bound_task(duration):\n",
        "    \"\"\"I/O-bound task that simulates waiting.\"\"\"\n",
        "    time.sleep(duration)\n",
        "    return f\"Completed after {duration} seconds\"\n",
        "\n",
        "# Sequential execution\n",
        "print(\"Sequential execution:\")\n",
        "start_time = time.time()\n",
        "results = []\n",
        "for i in range(3):\n",
        "    result = io_bound_task(1)\n",
        "    results.append(result)\n",
        "sequential_time = time.time() - start_time\n",
        "print(f\"Sequential time: {sequential_time:.2f} seconds\")\n",
        "print(f\"Results: {results}\")\n",
        "\n",
        "# Concurrent execution with threading\n",
        "print(\"\\nConcurrent execution with threading:\")\n",
        "start_time = time.time()\n",
        "threads = []\n",
        "results = []\n",
        "\n",
        "def thread_worker(duration):\n",
        "    result = io_bound_task(duration)\n",
        "    results.append(result)\n",
        "\n",
        "for i in range(3):\n",
        "    thread = threading.Thread(target=thread_worker, args=(1,))\n",
        "    threads.append(thread)\n",
        "    thread.start()\n",
        "\n",
        "for thread in threads:\n",
        "    thread.join()\n",
        "\n",
        "concurrent_time = time.time() - start_time\n",
        "print(f\"Concurrent time: {concurrent_time:.2f} seconds\")\n",
        "print(f\"Results: {results}\")\n",
        "\n",
        "# Parallel execution with multiprocessing\n",
        "print(\"\\nParallel execution with multiprocessing:\")\n",
        "start_time = time.time()\n",
        "with ProcessPoolExecutor(max_workers=3) as executor:\n",
        "    futures = [executor.submit(io_bound_task, 1) for _ in range(3)]\n",
        "    results = [future.result() for future in futures]\n",
        "parallel_time = time.time() - start_time\n",
        "print(f\"Parallel time: {parallel_time:.2f} seconds\")\n",
        "print(f\"Results: {results}\")\n",
        "\n",
        "# CPU-bound task comparison\n",
        "print(\"\\nCPU-bound task comparison:\")\n",
        "n = 1000000\n",
        "\n",
        "# Sequential\n",
        "start_time = time.time()\n",
        "sequential_result = cpu_bound_task(n)\n",
        "sequential_time = time.time() - start_time\n",
        "print(f\"Sequential CPU task: {sequential_time:.2f} seconds\")\n",
        "\n",
        "# Threading (limited by GIL)\n",
        "start_time = time.time()\n",
        "with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    futures = [executor.submit(cpu_bound_task, n//4) for _ in range(4)]\n",
        "    threading_results = [future.result() for future in futures]\n",
        "threading_time = time.time() - start_time\n",
        "print(f\"Threading CPU task: {threading_time:.2f} seconds\")\n",
        "\n",
        "# Multiprocessing (true parallelism)\n",
        "start_time = time.time()\n",
        "with ProcessPoolExecutor(max_workers=4) as executor:\n",
        "    futures = [executor.submit(cpu_bound_task, n//4) for _ in range(4)]\n",
        "    multiprocessing_results = [future.result() for future in futures]\n",
        "multiprocessing_time = time.time() - start_time\n",
        "print(f\"Multiprocessing CPU task: {multiprocessing_time:.2f} seconds\")\n",
        "\n",
        "# Performance comparison\n",
        "print(f\"\\nPerformance comparison:\")\n",
        "print(f\"Sequential: 1.00x\")\n",
        "print(f\"Threading: {sequential_time/threading_time:.2f}x\")\n",
        "print(f\"Multiprocessing: {sequential_time/multiprocessing_time:.2f}x\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Threading\n",
        "print(\"2. Threading\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "import threading\n",
        "import time\n",
        "import random\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import queue\n",
        "\n",
        "# Basic threading\n",
        "print(\"Basic threading:\")\n",
        "def worker(name, duration):\n",
        "    print(f\"Worker {name} starting\")\n",
        "    time.sleep(duration)\n",
        "    print(f\"Worker {name} finished\")\n",
        "\n",
        "# Create and start threads\n",
        "threads = []\n",
        "for i in range(3):\n",
        "    thread = threading.Thread(target=worker, args=(f\"Thread-{i+1}\", random.uniform(1, 3)))\n",
        "    threads.append(thread)\n",
        "    thread.start()\n",
        "\n",
        "# Wait for all threads to complete\n",
        "for thread in threads:\n",
        "    thread.join()\n",
        "\n",
        "print(\"All threads completed\")\n",
        "\n",
        "# Threading with shared data\n",
        "print(\"\\nThreading with shared data:\")\n",
        "counter = 0\n",
        "lock = threading.Lock()\n",
        "\n",
        "def increment_counter():\n",
        "    global counter\n",
        "    for _ in range(100000):\n",
        "        with lock:\n",
        "            counter += 1\n",
        "\n",
        "# Create threads\n",
        "threads = []\n",
        "for i in range(5):\n",
        "    thread = threading.Thread(target=increment_counter)\n",
        "    threads.append(thread)\n",
        "    thread.start()\n",
        "\n",
        "# Wait for completion\n",
        "for thread in threads:\n",
        "    thread.join()\n",
        "\n",
        "print(f\"Final counter value: {counter}\")\n",
        "\n",
        "# ThreadPoolExecutor\n",
        "print(\"\\nThreadPoolExecutor:\")\n",
        "def square_number(n):\n",
        "    return n ** 2\n",
        "\n",
        "# Submit tasks to thread pool\n",
        "with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    futures = [executor.submit(square_number, i) for i in range(10)]\n",
        "    results = [future.result() for future in futures]\n",
        "\n",
        "print(f\"Results: {results}\")\n",
        "\n",
        "# Producer-Consumer pattern\n",
        "print(\"\\nProducer-Consumer pattern:\")\n",
        "q = queue.Queue(maxsize=5)\n",
        "\n",
        "def producer():\n",
        "    for i in range(10):\n",
        "        q.put(f\"Item-{i}\")\n",
        "        print(f\"Produced: Item-{i}\")\n",
        "        time.sleep(0.1)\n",
        "\n",
        "def consumer():\n",
        "    while True:\n",
        "        try:\n",
        "            item = q.get(timeout=1)\n",
        "            print(f\"Consumed: {item}\")\n",
        "            time.sleep(0.2)\n",
        "            q.task_done()\n",
        "        except queue.Empty:\n",
        "            break\n",
        "\n",
        "# Start producer and consumer threads\n",
        "producer_thread = threading.Thread(target=producer)\n",
        "consumer_thread = threading.Thread(target=consumer)\n",
        "\n",
        "producer_thread.start()\n",
        "consumer_thread.start()\n",
        "\n",
        "producer_thread.join()\n",
        "consumer_thread.join()\n",
        "\n",
        "# Thread-safe data structures\n",
        "print(\"\\nThread-safe data structures:\")\n",
        "from collections import deque\n",
        "import collections\n",
        "\n",
        "# Thread-safe queue\n",
        "thread_safe_queue = queue.Queue()\n",
        "\n",
        "def queue_worker():\n",
        "    for i in range(5):\n",
        "        thread_safe_queue.put(f\"Data-{i}\")\n",
        "        time.sleep(0.1)\n",
        "\n",
        "def queue_reader():\n",
        "    while not thread_safe_queue.empty():\n",
        "        try:\n",
        "            data = thread_safe_queue.get(timeout=0.5)\n",
        "            print(f\"Read: {data}\")\n",
        "            thread_safe_queue.task_done()\n",
        "        except queue.Empty:\n",
        "            break\n",
        "\n",
        "# Start threads\n",
        "writer_thread = threading.Thread(target=queue_worker)\n",
        "reader_thread = threading.Thread(target=queue_reader)\n",
        "\n",
        "writer_thread.start()\n",
        "reader_thread.start()\n",
        "\n",
        "writer_thread.join()\n",
        "reader_thread.join()\n",
        "\n",
        "# Thread local storage\n",
        "print(\"\\nThread local storage:\")\n",
        "thread_local_data = threading.local()\n",
        "\n",
        "def thread_local_worker(thread_id):\n",
        "    thread_local_data.value = f\"Thread-{thread_id}\"\n",
        "    thread_local_data.counter = 0\n",
        "    \n",
        "    for i in range(3):\n",
        "        thread_local_data.counter += 1\n",
        "        print(f\"{thread_local_data.value}: counter = {thread_local_data.counter}\")\n",
        "        time.sleep(0.1)\n",
        "\n",
        "# Create threads with local storage\n",
        "threads = []\n",
        "for i in range(3):\n",
        "    thread = threading.Thread(target=thread_local_worker, args=(i+1,))\n",
        "    threads.append(thread)\n",
        "    thread.start()\n",
        "\n",
        "for thread in threads:\n",
        "    thread.join()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Multiprocessing\n",
        "print(\"3. Multiprocessing\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "import multiprocessing\n",
        "import time\n",
        "import os\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import multiprocessing as mp\n",
        "\n",
        "# Basic multiprocessing\n",
        "print(\"Basic multiprocessing:\")\n",
        "def worker_process(name, duration):\n",
        "    print(f\"Process {name} (PID: {os.getpid()}) starting\")\n",
        "    time.sleep(duration)\n",
        "    print(f\"Process {name} (PID: {os.getpid()}) finished\")\n",
        "    return f\"Result from {name}\"\n",
        "\n",
        "# Create and start processes\n",
        "processes = []\n",
        "for i in range(3):\n",
        "    process = multiprocessing.Process(target=worker_process, args=(f\"Process-{i+1}\", 2))\n",
        "    processes.append(process)\n",
        "    process.start()\n",
        "\n",
        "# Wait for all processes to complete\n",
        "for process in processes:\n",
        "    process.join()\n",
        "\n",
        "print(\"All processes completed\")\n",
        "\n",
        "# ProcessPoolExecutor\n",
        "print(\"\\nProcessPoolExecutor:\")\n",
        "def cpu_intensive_task(n):\n",
        "    \"\"\"CPU-intensive task that benefits from multiprocessing.\"\"\"\n",
        "    result = 0\n",
        "    for i in range(n):\n",
        "        result += i ** 2\n",
        "    return result\n",
        "\n",
        "# Submit tasks to process pool\n",
        "with ProcessPoolExecutor(max_workers=4) as executor:\n",
        "    futures = [executor.submit(cpu_intensive_task, 1000000) for _ in range(4)]\n",
        "    results = [future.result() for future in futures]\n",
        "\n",
        "print(f\"Results: {results}\")\n",
        "\n",
        "# Shared memory with multiprocessing\n",
        "print(\"\\nShared memory with multiprocessing:\")\n",
        "def worker_with_shared_data(shared_array, process_id):\n",
        "    \"\"\"Worker that modifies shared data.\"\"\"\n",
        "    for i in range(len(shared_array)):\n",
        "        shared_array[i] = process_id * 100 + i\n",
        "    print(f\"Process {process_id} modified shared array\")\n",
        "\n",
        "# Create shared array\n",
        "shared_array = multiprocessing.Array('i', 10)\n",
        "\n",
        "# Create processes\n",
        "processes = []\n",
        "for i in range(3):\n",
        "    process = multiprocessing.Process(target=worker_with_shared_data, args=(shared_array, i+1))\n",
        "    processes.append(process)\n",
        "    process.start()\n",
        "\n",
        "# Wait for completion\n",
        "for process in processes:\n",
        "    process.join()\n",
        "\n",
        "print(f\"Final shared array: {list(shared_array)}\")\n",
        "\n",
        "# Process communication with Queue\n",
        "print(\"\\nProcess communication with Queue:\")\n",
        "def producer_process(queue):\n",
        "    \"\"\"Producer process that puts data in queue.\"\"\"\n",
        "    for i in range(5):\n",
        "        queue.put(f\"Data-{i}\")\n",
        "        print(f\"Producer put: Data-{i}\")\n",
        "        time.sleep(0.1)\n",
        "\n",
        "def consumer_process(queue):\n",
        "    \"\"\"Consumer process that gets data from queue.\"\"\"\n",
        "    while True:\n",
        "        try:\n",
        "            data = queue.get(timeout=1)\n",
        "            print(f\"Consumer got: {data}\")\n",
        "            time.sleep(0.2)\n",
        "        except:\n",
        "            break\n",
        "\n",
        "# Create queue for inter-process communication\n",
        "process_queue = multiprocessing.Queue()\n",
        "\n",
        "# Create producer and consumer processes\n",
        "producer = multiprocessing.Process(target=producer_process, args=(process_queue,))\n",
        "consumer = multiprocessing.Process(target=consumer_process, args=(process_queue,))\n",
        "\n",
        "# Start processes\n",
        "producer.start()\n",
        "consumer.start()\n",
        "\n",
        "# Wait for completion\n",
        "producer.join()\n",
        "consumer.join()\n",
        "\n",
        "# Process synchronization\n",
        "print(\"\\nProcess synchronization:\")\n",
        "def synchronized_worker(lock, counter, process_id):\n",
        "    \"\"\"Worker that uses process lock for synchronization.\"\"\"\n",
        "    for _ in range(3):\n",
        "        with lock:\n",
        "            counter.value += 1\n",
        "            print(f\"Process {process_id}: counter = {counter.value}\")\n",
        "        time.sleep(0.1)\n",
        "\n",
        "# Create shared counter and lock\n",
        "shared_counter = multiprocessing.Value('i', 0)\n",
        "process_lock = multiprocessing.Lock()\n",
        "\n",
        "# Create processes\n",
        "processes = []\n",
        "for i in range(3):\n",
        "    process = multiprocessing.Process(target=synchronized_worker, args=(process_lock, shared_counter, i+1))\n",
        "    processes.append(process)\n",
        "    process.start()\n",
        "\n",
        "# Wait for completion\n",
        "for process in processes:\n",
        "    process.join()\n",
        "\n",
        "print(f\"Final counter value: {shared_counter.value}\")\n",
        "\n",
        "# Process pool with map\n",
        "print(\"\\nProcess pool with map:\")\n",
        "def square_number(n):\n",
        "    return n ** 2\n",
        "\n",
        "# Use process pool to map function over data\n",
        "with ProcessPoolExecutor(max_workers=4) as executor:\n",
        "    numbers = list(range(10))\n",
        "    results = list(executor.map(square_number, numbers))\n",
        "\n",
        "print(f\"Numbers: {numbers}\")\n",
        "print(f\"Squared: {results}\")\n",
        "\n",
        "# Process vs Thread performance comparison\n",
        "print(\"\\nProcess vs Thread performance comparison:\")\n",
        "def cpu_bound_task(n):\n",
        "    \"\"\"CPU-bound task for performance comparison.\"\"\"\n",
        "    result = 0\n",
        "    for i in range(n):\n",
        "        result += i ** 2\n",
        "    return result\n",
        "\n",
        "# Test with different numbers of workers\n",
        "n = 1000000\n",
        "workers = [1, 2, 4, 8]\n",
        "\n",
        "print(\"Workers | Threading | Multiprocessing\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "for num_workers in workers:\n",
        "    # Threading\n",
        "    start_time = time.time()\n",
        "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "        futures = [executor.submit(cpu_bound_task, n//num_workers) for _ in range(num_workers)]\n",
        "        results = [future.result() for future in futures]\n",
        "    threading_time = time.time() - start_time\n",
        "    \n",
        "    # Multiprocessing\n",
        "    start_time = time.time()\n",
        "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
        "        futures = [executor.submit(cpu_bound_task, n//num_workers) for _ in range(num_workers)]\n",
        "        results = [future.result() for future in futures]\n",
        "    multiprocessing_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"{num_workers:7} | {threading_time:8.2f}s | {multiprocessing_time:13.2f}s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Asyncio\n",
        "print(\"4. Asyncio\")\n",
        "print(\"-\" * 15)\n",
        "\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import time\n",
        "import random\n",
        "\n",
        "# Basic asyncio\n",
        "print(\"Basic asyncio:\")\n",
        "async def async_task(name, duration):\n",
        "    print(f\"Task {name} starting\")\n",
        "    await asyncio.sleep(duration)\n",
        "    print(f\"Task {name} finished\")\n",
        "    return f\"Result from {name}\"\n",
        "\n",
        "async def main():\n",
        "    # Create tasks\n",
        "    tasks = [\n",
        "        async_task(\"Task-1\", 1),\n",
        "        async_task(\"Task-2\", 2),\n",
        "        async_task(\"Task-3\", 1.5)\n",
        "    ]\n",
        "    \n",
        "    # Run tasks concurrently\n",
        "    results = await asyncio.gather(*tasks)\n",
        "    print(f\"Results: {results}\")\n",
        "\n",
        "# Run the async main function\n",
        "asyncio.run(main())\n",
        "\n",
        "# Async with asyncio.gather\n",
        "print(\"\\nAsync with asyncio.gather:\")\n",
        "async def fetch_data(url):\n",
        "    \"\"\"Simulate fetching data from a URL.\"\"\"\n",
        "    await asyncio.sleep(random.uniform(0.5, 2.0))\n",
        "    return f\"Data from {url}\"\n",
        "\n",
        "async def gather_example():\n",
        "    urls = [\"http://example.com\", \"http://test.com\", \"http://demo.com\"]\n",
        "    tasks = [fetch_data(url) for url in urls]\n",
        "    results = await asyncio.gather(*tasks)\n",
        "    return results\n",
        "\n",
        "# Run gather example\n",
        "results = asyncio.run(gather_example())\n",
        "print(f\"Gathered results: {results}\")\n",
        "\n",
        "# Async with asyncio.create_task\n",
        "print(\"\\nAsync with asyncio.create_task:\")\n",
        "async def create_task_example():\n",
        "    # Create tasks\n",
        "    task1 = asyncio.create_task(fetch_data(\"http://api1.com\"))\n",
        "    task2 = asyncio.create_task(fetch_data(\"http://api2.com\"))\n",
        "    task3 = asyncio.create_task(fetch_data(\"http://api3.com\"))\n",
        "    \n",
        "    # Wait for tasks to complete\n",
        "    result1 = await task1\n",
        "    result2 = await task2\n",
        "    result3 = await task3\n",
        "    \n",
        "    return [result1, result2, result3]\n",
        "\n",
        "# Run create task example\n",
        "results = asyncio.run(create_task_example())\n",
        "print(f\"Task results: {results}\")\n",
        "\n",
        "# Async with asyncio.wait\n",
        "print(\"\\nAsync with asyncio.wait:\")\n",
        "async def wait_example():\n",
        "    tasks = [\n",
        "        asyncio.create_task(fetch_data(\"http://slow.com\")),\n",
        "        asyncio.create_task(fetch_data(\"http://fast.com\")),\n",
        "        asyncio.create_task(fetch_data(\"http://medium.com\"))\n",
        "    ]\n",
        "    \n",
        "    # Wait for all tasks to complete\n",
        "    done, pending = await asyncio.wait(tasks, return_when=asyncio.ALL_COMPLETED)\n",
        "    \n",
        "    results = []\n",
        "    for task in done:\n",
        "        results.append(task.result())\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run wait example\n",
        "results = asyncio.run(wait_example())\n",
        "print(f\"Wait results: {results}\")\n",
        "\n",
        "# Async with asyncio.as_completed\n",
        "print(\"\\nAsync with asyncio.as_completed:\")\n",
        "async def as_completed_example():\n",
        "    tasks = [\n",
        "        asyncio.create_task(fetch_data(\"http://api1.com\")),\n",
        "        asyncio.create_task(fetch_data(\"http://api2.com\")),\n",
        "        asyncio.create_task(fetch_data(\"http://api3.com\"))\n",
        "    ]\n",
        "    \n",
        "    results = []\n",
        "    for coro in asyncio.as_completed(tasks):\n",
        "        result = await coro\n",
        "        results.append(result)\n",
        "        print(f\"Completed: {result}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run as_completed example\n",
        "results = asyncio.run(as_completed_example())\n",
        "print(f\"As completed results: {results}\")\n",
        "\n",
        "# Async with asyncio.timeout\n",
        "print(\"\\nAsync with asyncio.timeout:\")\n",
        "async def timeout_example():\n",
        "    try:\n",
        "        async with asyncio.timeout(1.0):\n",
        "            await fetch_data(\"http://slow.com\")\n",
        "    except asyncio.TimeoutError:\n",
        "        print(\"Task timed out!\")\n",
        "\n",
        "# Run timeout example\n",
        "asyncio.run(timeout_example())\n",
        "\n",
        "# Async with asyncio.shield\n",
        "print(\"\\nAsync with asyncio.shield:\")\n",
        "async def shield_example():\n",
        "    async def long_running_task():\n",
        "        await asyncio.sleep(2)\n",
        "        return \"Long task completed\"\n",
        "    \n",
        "    # Shield the task from cancellation\n",
        "    shielded_task = asyncio.shield(long_running_task())\n",
        "    \n",
        "    # Cancel the shielded task\n",
        "    shielded_task.cancel()\n",
        "    \n",
        "    try:\n",
        "        result = await shielded_task\n",
        "        print(f\"Shielded result: {result}\")\n",
        "    except asyncio.CancelledError:\n",
        "        print(\"Shielded task was cancelled\")\n",
        "\n",
        "# Run shield example\n",
        "asyncio.run(shield_example())\n",
        "\n",
        "# Async with asyncio.gather and return_exceptions\n",
        "print(\"\\nAsync with asyncio.gather and return_exceptions:\")\n",
        "async def gather_with_exceptions():\n",
        "    async def task_with_exception():\n",
        "        await asyncio.sleep(0.5)\n",
        "        raise ValueError(\"Task failed!\")\n",
        "    \n",
        "    async def normal_task():\n",
        "        await asyncio.sleep(1)\n",
        "        return \"Normal task completed\"\n",
        "    \n",
        "    tasks = [\n",
        "        task_with_exception(),\n",
        "        normal_task()\n",
        "    ]\n",
        "    \n",
        "    # Gather with return_exceptions=True\n",
        "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "    \n",
        "    for i, result in enumerate(results):\n",
        "        if isinstance(result, Exception):\n",
        "            print(f\"Task {i} failed: {result}\")\n",
        "        else:\n",
        "            print(f\"Task {i} succeeded: {result}\")\n",
        "\n",
        "# Run gather with exceptions example\n",
        "asyncio.run(gather_with_exceptions())\n",
        "\n",
        "# Async with asyncio.gather and return_when\n",
        "print(\"\\nAsync with asyncio.gather and return_when:\")\n",
        "async def gather_with_return_when():\n",
        "    tasks = [\n",
        "        asyncio.create_task(fetch_data(\"http://fast.com\")),\n",
        "        asyncio.create_task(fetch_data(\"http://slow.com\")),\n",
        "        asyncio.create_task(fetch_data(\"http://medium.com\"))\n",
        "    ]\n",
        "    \n",
        "    # Wait for first task to complete\n",
        "    done, pending = await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)\n",
        "    \n",
        "    print(f\"Completed tasks: {len(done)}\")\n",
        "    print(f\"Pending tasks: {len(pending)}\")\n",
        "    \n",
        "    # Cancel pending tasks\n",
        "    for task in pending:\n",
        "        task.cancel()\n",
        "    \n",
        "    # Wait for cancelled tasks\n",
        "    await asyncio.gather(*pending, return_exceptions=True)\n",
        "\n",
        "# Run gather with return_when example\n",
        "asyncio.run(gather_with_return_when())\n",
        "\n",
        "# Async with asyncio.gather and timeout\n",
        "print(\"\\nAsync with asyncio.gather and timeout:\")\n",
        "async def gather_with_timeout():\n",
        "    tasks = [\n",
        "        asyncio.create_task(fetch_data(\"http://slow.com\")),\n",
        "        asyncio.create_task(fetch_data(\"http://very-slow.com\")),\n",
        "        asyncio.create_task(fetch_data(\"http://fast.com\"))\n",
        "    ]\n",
        "    \n",
        "    try:\n",
        "        async with asyncio.timeout(1.5):\n",
        "            results = await asyncio.gather(*tasks)\n",
        "            print(f\"All tasks completed: {results}\")\n",
        "    except asyncio.TimeoutError:\n",
        "        print(\"Gather timed out!\")\n",
        "        \n",
        "        # Check which tasks completed\n",
        "        for i, task in enumerate(tasks):\n",
        "            if task.done():\n",
        "                print(f\"Task {i} completed: {task.result()}\")\n",
        "            else:\n",
        "                print(f\"Task {i} still running\")\n",
        "\n",
        "# Run gather with timeout example\n",
        "asyncio.run(gather_with_timeout())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Performance Comparison\n",
        "print(\"5. Performance Comparison\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "import time\n",
        "import threading\n",
        "import multiprocessing\n",
        "import asyncio\n",
        "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
        "\n",
        "# Performance comparison functions\n",
        "def cpu_bound_task(n):\n",
        "    \"\"\"CPU-bound task for performance comparison.\"\"\"\n",
        "    result = 0\n",
        "    for i in range(n):\n",
        "        result += i ** 2\n",
        "    return result\n",
        "\n",
        "async def async_cpu_bound_task(n):\n",
        "    \"\"\"Async CPU-bound task.\"\"\"\n",
        "    result = 0\n",
        "    for i in range(n):\n",
        "        result += i ** 2\n",
        "    return result\n",
        "\n",
        "def io_bound_task(duration):\n",
        "    \"\"\"I/O-bound task for performance comparison.\"\"\"\n",
        "    time.sleep(duration)\n",
        "    return f\"Completed after {duration} seconds\"\n",
        "\n",
        "async def async_io_bound_task(duration):\n",
        "    \"\"\"Async I/O-bound task.\"\"\"\n",
        "    await asyncio.sleep(duration)\n",
        "    return f\"Completed after {duration} seconds\"\n",
        "\n",
        "# CPU-bound performance comparison\n",
        "print(\"CPU-bound performance comparison:\")\n",
        "n = 1000000\n",
        "workers = [1, 2, 4, 8]\n",
        "\n",
        "print(\"Workers | Sequential | Threading | Multiprocessing | Asyncio\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for num_workers in workers:\n",
        "    # Sequential\n",
        "    start_time = time.time()\n",
        "    for _ in range(num_workers):\n",
        "        cpu_bound_task(n//num_workers)\n",
        "    sequential_time = time.time() - start_time\n",
        "    \n",
        "    # Threading\n",
        "    start_time = time.time()\n",
        "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "        futures = [executor.submit(cpu_bound_task, n//num_workers) for _ in range(num_workers)]\n",
        "        results = [future.result() for future in futures]\n",
        "    threading_time = time.time() - start_time\n",
        "    \n",
        "    # Multiprocessing\n",
        "    start_time = time.time()\n",
        "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
        "        futures = [executor.submit(cpu_bound_task, n//num_workers) for _ in range(num_workers)]\n",
        "        results = [future.result() for future in futures]\n",
        "    multiprocessing_time = time.time() - start_time\n",
        "    \n",
        "    # Asyncio\n",
        "    async def async_cpu_test():\n",
        "        tasks = [async_cpu_bound_task(n//num_workers) for _ in range(num_workers)]\n",
        "        return await asyncio.gather(*tasks)\n",
        "    \n",
        "    start_time = time.time()\n",
        "    asyncio.run(async_cpu_test())\n",
        "    asyncio_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"{num_workers:7} | {sequential_time:9.2f}s | {threading_time:8.2f}s | {multiprocessing_time:13.2f}s | {asyncio_time:6.2f}s\")\n",
        "\n",
        "# I/O-bound performance comparison\n",
        "print(\"\\nI/O-bound performance comparison:\")\n",
        "duration = 1.0\n",
        "workers = [1, 2, 4, 8]\n",
        "\n",
        "print(\"Workers | Sequential | Threading | Multiprocessing | Asyncio\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for num_workers in workers:\n",
        "    # Sequential\n",
        "    start_time = time.time()\n",
        "    for _ in range(num_workers):\n",
        "        io_bound_task(duration)\n",
        "    sequential_time = time.time() - start_time\n",
        "    \n",
        "    # Threading\n",
        "    start_time = time.time()\n",
        "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "        futures = [executor.submit(io_bound_task, duration) for _ in range(num_workers)]\n",
        "        results = [future.result() for future in futures]\n",
        "    threading_time = time.time() - start_time\n",
        "    \n",
        "    # Multiprocessing\n",
        "    start_time = time.time()\n",
        "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
        "        futures = [executor.submit(io_bound_task, duration) for _ in range(num_workers)]\n",
        "        results = [future.result() for future in futures]\n",
        "    multiprocessing_time = time.time() - start_time\n",
        "    \n",
        "    # Asyncio\n",
        "    async def async_io_test():\n",
        "        tasks = [async_io_bound_task(duration) for _ in range(num_workers)]\n",
        "        return await asyncio.gather(*tasks)\n",
        "    \n",
        "    start_time = time.time()\n",
        "    asyncio.run(async_io_test())\n",
        "    asyncio_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"{num_workers:7} | {sequential_time:9.2f}s | {threading_time:8.2f}s | {multiprocessing_time:13.2f}s | {asyncio_time:6.2f}s\")\n",
        "\n",
        "# Memory usage comparison\n",
        "print(\"\\nMemory usage comparison:\")\n",
        "import psutil\n",
        "import os\n",
        "\n",
        "def get_memory_usage():\n",
        "    \"\"\"Get current memory usage in MB.\"\"\"\n",
        "    process = psutil.Process(os.getpid())\n",
        "    return process.memory_info().rss / 1024 / 1024\n",
        "\n",
        "# Test memory usage for different approaches\n",
        "print(\"Approach | Memory Usage (MB)\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Sequential\n",
        "memory_before = get_memory_usage()\n",
        "for _ in range(4):\n",
        "    cpu_bound_task(100000)\n",
        "memory_after = get_memory_usage()\n",
        "print(f\"Sequential | {memory_after - memory_before:.2f}\")\n",
        "\n",
        "# Threading\n",
        "memory_before = get_memory_usage()\n",
        "with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    futures = [executor.submit(cpu_bound_task, 100000) for _ in range(4)]\n",
        "    results = [future.result() for future in futures]\n",
        "memory_after = get_memory_usage()\n",
        "print(f\"Threading | {memory_after - memory_before:.2f}\")\n",
        "\n",
        "# Multiprocessing\n",
        "memory_before = get_memory_usage()\n",
        "with ProcessPoolExecutor(max_workers=4) as executor:\n",
        "    futures = [executor.submit(cpu_bound_task, 100000) for _ in range(4)]\n",
        "    results = [future.result() for future in futures]\n",
        "memory_after = get_memory_usage()\n",
        "print(f\"Multiprocessing | {memory_after - memory_before:.2f}\")\n",
        "\n",
        "# Asyncio\n",
        "memory_before = get_memory_usage()\n",
        "async def async_memory_test():\n",
        "    tasks = [async_cpu_bound_task(100000) for _ in range(4)]\n",
        "    return await asyncio.gather(*tasks)\n",
        "\n",
        "asyncio.run(async_memory_test())\n",
        "memory_after = get_memory_usage()\n",
        "print(f\"Asyncio | {memory_after - memory_before:.2f}\")\n",
        "\n",
        "# Scalability analysis\n",
        "print(\"\\nScalability analysis:\")\n",
        "print(\"CPU-bound tasks:\")\n",
        "print(\"- Sequential: O(n) - linear scaling\")\n",
        "print(\"- Threading: O(n) - limited by GIL\")\n",
        "print(\"- Multiprocessing: O(n/k) - scales with CPU cores\")\n",
        "print(\"- Asyncio: O(n) - single-threaded\")\n",
        "\n",
        "print(\"\\nI/O-bound tasks:\")\n",
        "print(\"- Sequential: O(n) - linear scaling\")\n",
        "print(\"- Threading: O(n/k) - good for I/O\")\n",
        "print(\"- Multiprocessing: O(n/k) - good for I/O\")\n",
        "print(\"- Asyncio: O(n/k) - excellent for I/O\")\n",
        "\n",
        "# Best practices summary\n",
        "print(\"\\nBest practices summary:\")\n",
        "print(\"1. Use sequential execution for simple, small tasks\")\n",
        "print(\"2. Use threading for I/O-bound tasks\")\n",
        "print(\"3. Use multiprocessing for CPU-bound tasks\")\n",
        "print(\"4. Use asyncio for I/O-bound tasks with many concurrent operations\")\n",
        "print(\"5. Consider memory usage and overhead\")\n",
        "print(\"6. Profile your code to identify bottlenecks\")\n",
        "print(\"7. Use appropriate synchronization primitives\")\n",
        "print(\"8. Handle exceptions properly in concurrent code\")\n",
        "print(\"9. Consider the Global Interpreter Lock (GIL) limitations\")\n",
        "print(\"10. Use context managers for resource management\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practice Exercises\n",
        "\n",
        "### Exercise 1: Threading vs Multiprocessing\n",
        "Create a program that compares the performance of threading and multiprocessing for both CPU-bound and I/O-bound tasks. Measure execution time and memory usage for different numbers of workers.\n",
        "\n",
        "### Exercise 2: Async Web Scraper\n",
        "Build an async web scraper that fetches data from multiple URLs concurrently. Use asyncio and aiohttp to implement the scraper with proper error handling and rate limiting.\n",
        "\n",
        "### Exercise 3: Producer-Consumer Pattern\n",
        "Implement a producer-consumer pattern using both threading and multiprocessing. The producer should generate data, and the consumer should process it. Use queues for communication between threads/processes.\n",
        "\n",
        "### Exercise 4: Thread Pool vs Process Pool\n",
        "Create a program that uses both ThreadPoolExecutor and ProcessPoolExecutor to process a list of tasks. Compare the performance and choose the appropriate executor based on the task type.\n",
        "\n",
        "### Exercise 5: Async Task Management\n",
        "Implement an async task manager that can handle multiple tasks with different priorities. Use asyncio to manage task execution, cancellation, and error handling.\n",
        "\n",
        "### Exercise 6: Concurrent File Processing\n",
        "Build a program that processes multiple files concurrently using different approaches (threading, multiprocessing, asyncio). Compare the performance and choose the best approach for your use case.\n",
        "\n",
        "### Exercise 7: Real-time Data Processing\n",
        "Create a real-time data processing system that uses threading or asyncio to handle incoming data streams. Implement proper synchronization and error handling.\n",
        "\n",
        "### Exercise 8: Performance Profiling\n",
        "Profile your concurrent code to identify bottlenecks and optimize performance. Use tools like cProfile, line_profiler, or memory_profiler to analyze your code.\n",
        "\n",
        "### Exercise 9: Error Handling in Concurrent Code\n",
        "Implement proper error handling for concurrent code. Handle exceptions, timeouts, and cancellation gracefully.\n",
        "\n",
        "### Exercise 10: Custom Thread Pool\n",
        "Create a custom thread pool implementation that provides more control over thread management. Implement features like dynamic scaling, task prioritization, and resource management.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this lesson, we've covered the fundamentals of concurrency and parallelism in Python:\n",
        "\n",
        "### Key Concepts Covered:\n",
        "1. **Concurrency vs Parallelism**: Understanding the difference between concurrent and parallel execution\n",
        "2. **Threading**: Using threads for I/O-bound tasks, understanding the GIL limitations\n",
        "3. **Multiprocessing**: Using processes for CPU-bound tasks, true parallelism\n",
        "4. **Asyncio**: Asynchronous programming for I/O-bound tasks with many concurrent operations\n",
        "5. **Performance Comparison**: Comparing different approaches for different types of tasks\n",
        "\n",
        "### Key Takeaways:\n",
        "- **Use threading for I/O-bound tasks** where you need to wait for external resources\n",
        "- **Use multiprocessing for CPU-bound tasks** where you need true parallelism\n",
        "- **Use asyncio for I/O-bound tasks** with many concurrent operations\n",
        "- **Consider the GIL limitations** when choosing between threading and multiprocessing\n",
        "- **Profile your code** to identify bottlenecks and optimize performance\n",
        "- **Handle exceptions properly** in concurrent code\n",
        "- **Use appropriate synchronization primitives** to avoid race conditions\n",
        "\n",
        "### Next Steps:\n",
        "- Practice with the exercises provided\n",
        "- Explore advanced topics like distributed computing\n",
        "- Learn about specific libraries like Celery for task queues\n",
        "- Study design patterns for concurrent programming\n",
        "- Experiment with different approaches for your specific use cases\n",
        "\n",
        "### Resources for Further Learning:\n",
        "- [Python Threading Documentation](https://docs.python.org/3/library/threading.html)\n",
        "- [Python Multiprocessing Documentation](https://docs.python.org/3/library/multiprocessing.html)\n",
        "- [Python Asyncio Documentation](https://docs.python.org/3/library/asyncio.html)\n",
        "- [Concurrent Programming in Python](https://realpython.com/python-concurrency/)\n",
        "- [AsyncIO Tutorial](https://docs.python.org/3/library/asyncio-task.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Concurrency & Parallelism - Working with Threads and Processes\n",
        "\n",
        "Welcome to the third lesson of the Advanced Level! In this lesson, you'll learn how to write concurrent and parallel programs in Python.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this lesson, you will be able to:\n",
        "- Understand the difference between concurrency and parallelism\n",
        "- Use threading for I/O-bound tasks\n",
        "- Use multiprocessing for CPU-bound tasks\n",
        "- Work with asyncio for asynchronous programming\n",
        "- Understand the Global Interpreter Lock (GIL)\n",
        "- Write efficient concurrent programs\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Concurrency vs Parallelism](#concurrency-vs-parallelism)\n",
        "2. [Threading](#threading)\n",
        "3. [Multiprocessing](#multiprocessing)\n",
        "4. [Asyncio](#asyncio)\n",
        "5. [Global Interpreter Lock](#global-interpreter-lock)\n",
        "6. [Best Practices](#best-practices)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Concurrency vs Parallelism\n",
        "\n",
        "Understanding the difference between concurrency and parallelism is crucial for writing efficient programs.\n",
        "\n",
        "### Concurrency\n",
        "- **Definition**: Multiple tasks making progress over the same period\n",
        "- **Use Case**: I/O-bound operations (file operations, network requests)\n",
        "- **Python Tools**: `threading`, `asyncio`\n",
        "- **Example**: Downloading multiple files simultaneously\n",
        "\n",
        "### Parallelism\n",
        "- **Definition**: Multiple tasks executing simultaneously\n",
        "- **Use Case**: CPU-bound operations (mathematical calculations)\n",
        "- **Python Tools**: `multiprocessing`\n",
        "- **Example**: Processing large datasets with multiple CPU cores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Threading Examples\n",
        "import threading\n",
        "import time\n",
        "import requests\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import queue\n",
        "\n",
        "# Basic threading example\n",
        "def worker(name, delay):\n",
        "    \"\"\"A simple worker function.\"\"\"\n",
        "    print(f\"Worker {name} starting\")\n",
        "    time.sleep(delay)\n",
        "    print(f\"Worker {name} finished\")\n",
        "\n",
        "print(\"Basic Threading Example\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "# Create and start threads\n",
        "threads = []\n",
        "for i in range(3):\n",
        "    t = threading.Thread(target=worker, args=(f\"Thread-{i+1}\", 2))\n",
        "    threads.append(t)\n",
        "    t.start()\n",
        "\n",
        "# Wait for all threads to complete\n",
        "for t in threads:\n",
        "    t.join()\n",
        "\n",
        "print(\"All threads completed!\")\n",
        "\n",
        "# Threading with shared data\n",
        "print(f\"\\nThreading with Shared Data\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "# Shared counter\n",
        "counter = 0\n",
        "lock = threading.Lock()\n",
        "\n",
        "def increment_counter():\n",
        "    global counter\n",
        "    for _ in range(100000):\n",
        "        with lock:\n",
        "            counter += 1\n",
        "\n",
        "# Without lock (race condition)\n",
        "counter_no_lock = 0\n",
        "\n",
        "def increment_no_lock():\n",
        "    global counter_no_lock\n",
        "    for _ in range(100000):\n",
        "        counter_no_lock += 1\n",
        "\n",
        "# Test with lock\n",
        "threads_with_lock = []\n",
        "for _ in range(3):\n",
        "    t = threading.Thread(target=increment_counter)\n",
        "    threads_with_lock.append(t)\n",
        "    t.start()\n",
        "\n",
        "for t in threads_with_lock:\n",
        "    t.join()\n",
        "\n",
        "# Test without lock\n",
        "threads_no_lock = []\n",
        "for _ in range(3):\n",
        "    t = threading.Thread(target=increment_no_lock)\n",
        "    threads_no_lock.append(t)\n",
        "    t.start()\n",
        "\n",
        "for t in threads_no_lock:\n",
        "    t.join()\n",
        "\n",
        "print(f\"Counter with lock: {counter}\")\n",
        "print(f\"Counter without lock: {counter_no_lock}\")\n",
        "print(f\"Expected: 300000\")\n",
        "\n",
        "# ThreadPoolExecutor example\n",
        "print(f\"\\nThreadPoolExecutor Example\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "def fetch_url(url):\n",
        "    \"\"\"Simulate fetching a URL.\"\"\"\n",
        "    time.sleep(1)  # Simulate network delay\n",
        "    return f\"Data from {url}\"\n",
        "\n",
        "urls = [\n",
        "    \"https://example.com\",\n",
        "    \"https://python.org\",\n",
        "    \"https://github.com\",\n",
        "    \"https://stackoverflow.com\"\n",
        "]\n",
        "\n",
        "# Sequential execution\n",
        "start_time = time.time()\n",
        "sequential_results = []\n",
        "for url in urls:\n",
        "    result = fetch_url(url)\n",
        "    sequential_results.append(result)\n",
        "sequential_time = time.time() - start_time\n",
        "\n",
        "# Concurrent execution\n",
        "start_time = time.time()\n",
        "with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    concurrent_results = list(executor.map(fetch_url, urls))\n",
        "concurrent_time = time.time() - start_time\n",
        "\n",
        "print(f\"Sequential time: {sequential_time:.2f} seconds\")\n",
        "print(f\"Concurrent time: {concurrent_time:.2f} seconds\")\n",
        "print(f\"Speedup: {sequential_time/concurrent_time:.2f}x\")\n",
        "\n",
        "# Producer-Consumer pattern\n",
        "print(f\"\\nProducer-Consumer Pattern\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "def producer(q, items):\n",
        "    \"\"\"Producer function.\"\"\"\n",
        "    for item in items:\n",
        "        q.put(item)\n",
        "        print(f\"Produced: {item}\")\n",
        "        time.sleep(0.5)\n",
        "    q.put(None)  # Signal end\n",
        "\n",
        "def consumer(q, name):\n",
        "    \"\"\"Consumer function.\"\"\"\n",
        "    while True:\n",
        "        item = q.get()\n",
        "        if item is None:\n",
        "            break\n",
        "        print(f\"Consumer {name} consumed: {item}\")\n",
        "        time.sleep(0.3)\n",
        "        q.task_done()\n",
        "\n",
        "# Create queue and threads\n",
        "q = queue.Queue()\n",
        "items = [\"item1\", \"item2\", \"item3\", \"item4\", \"item5\"]\n",
        "\n",
        "producer_thread = threading.Thread(target=producer, args=(q, items))\n",
        "consumer_thread = threading.Thread(target=consumer, args=(q, \"A\"))\n",
        "\n",
        "producer_thread.start()\n",
        "consumer_thread.start()\n",
        "\n",
        "producer_thread.join()\n",
        "consumer_thread.join()\n",
        "\n",
        "print(\"Producer-Consumer completed!\")\n",
        "\n",
        "# Thread-safe data structures\n",
        "print(f\"\\nThread-Safe Data Structures\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "# Thread-safe queue\n",
        "thread_safe_queue = queue.Queue()\n",
        "\n",
        "def producer_safe(q, name, count):\n",
        "    \"\"\"Thread-safe producer.\"\"\"\n",
        "    for i in range(count):\n",
        "        item = f\"{name}-{i}\"\n",
        "        q.put(item)\n",
        "        print(f\"Producer {name} put: {item}\")\n",
        "        time.sleep(random.uniform(0.1, 0.3))\n",
        "\n",
        "def consumer_safe(q, name):\n",
        "    \"\"\"Thread-safe consumer.\"\"\"\n",
        "    while True:\n",
        "        try:\n",
        "            item = q.get(timeout=1)\n",
        "            print(f\"Consumer {name} got: {item}\")\n",
        "            time.sleep(random.uniform(0.1, 0.3))\n",
        "            q.task_done()\n",
        "        except queue.Empty:\n",
        "            break\n",
        "\n",
        "# Create multiple producers and consumers\n",
        "producers = []\n",
        "consumers = []\n",
        "\n",
        "# Start producers\n",
        "for i in range(2):\n",
        "    p = threading.Thread(target=producer_safe, args=(thread_safe_queue, f\"P{i+1}\", 3))\n",
        "    producers.append(p)\n",
        "    p.start()\n",
        "\n",
        "# Start consumers\n",
        "for i in range(2):\n",
        "    c = threading.Thread(target=consumer_safe, args=(thread_safe_queue, f\"C{i+1}\"))\n",
        "    consumers.append(c)\n",
        "    c.start()\n",
        "\n",
        "# Wait for producers to finish\n",
        "for p in producers:\n",
        "    p.join()\n",
        "\n",
        "# Wait for queue to be empty\n",
        "thread_safe_queue.join()\n",
        "\n",
        "# Stop consumers\n",
        "for c in consumers:\n",
        "    c.join()\n",
        "\n",
        "print(\"Thread-safe operations completed!\")\n",
        "\n",
        "# Thread local storage\n",
        "print(f\"\\nThread Local Storage\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "thread_local_data = threading.local()\n",
        "\n",
        "def worker_with_local(name):\n",
        "    \"\"\"Worker with thread-local data.\"\"\"\n",
        "    thread_local_data.name = name\n",
        "    thread_local_data.counter = 0\n",
        "    \n",
        "    for i in range(3):\n",
        "        thread_local_data.counter += 1\n",
        "        print(f\"Thread {thread_local_data.name}: counter = {thread_local_data.counter}\")\n",
        "        time.sleep(0.1)\n",
        "\n",
        "# Create threads with local storage\n",
        "local_threads = []\n",
        "for i in range(3):\n",
        "    t = threading.Thread(target=worker_with_local, args=(f\"Local-{i+1}\",))\n",
        "    local_threads.append(t)\n",
        "    t.start()\n",
        "\n",
        "for t in local_threads:\n",
        "    t.join()\n",
        "\n",
        "print(\"Thread local storage completed!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
