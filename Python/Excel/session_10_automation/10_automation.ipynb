{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Session 10: Automation and Batch Processing\n",
        "\n",
        "Welcome to this interactive session! This notebook contains all the examples from the Python script.\n",
        "\n",
        "## ðŸ“‹ How to Use This Notebook\n",
        "\n",
        "1. Run cells sequentially using Shift+Enter\n",
        "2. Modify the code and experiment\n",
        "3. Check the output after each cell\n",
        "4. Refer to the markdown guide for detailed explanations\n",
        "\n",
        "## ðŸš€ Let's Begin!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Session 10: Automation and Batch Processing - Comprehensive Examples\n",
        "====================================================================\n",
        "\n",
        "This script demonstrates automation workflows, batch processing,\n",
        "and building robust Excel automation systems.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "import logging\n",
        "import sys\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Session 10: Automation and Batch Processing\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "# Create output directories\n",
        "output_dir = Path(__file__).parent / 'sample_files'\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "reports_dir = Path(__file__).parent / 'generated_reports'\n",
        "reports_dir.mkdir(exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. SETUP LOGGING\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"1. SETTING UP LOGGING\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Setup logging\n",
        "log_file = output_dir / f'automation_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(log_file),\n",
        "        logging.StreamHandler(sys.stdout)\n",
        "    ]\n",
        ")\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.info(\"Automation session started\")\n",
        "print(f\"âœ“ Logging configured: {log_file.name}\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. CREATE SAMPLE DATA FILES\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"2. CREATING SAMPLE DATA FILES\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "def create_daily_sales_file(date, output_dir):\n",
        "    \"\"\"Create daily sales file\"\"\"\n",
        "    np.random.seed(int(date.strftime('%Y%m%d')))\n",
        "    \n",
        "    df = pd.DataFrame({\n",
        "        'Date': [date] * 20,\n",
        "        'Product': np.random.choice(['Laptop', 'Mouse', 'Keyboard', 'Monitor'], 20),\n",
        "        'Region': np.random.choice(['North', 'South', 'East', 'West'], 20),\n",
        "        'Sales': np.random.randint(100, 5000, 20),\n",
        "        'Quantity': np.random.randint(1, 20, 20)\n",
        "    })\n",
        "    \n",
        "    filename = output_dir / f'daily_sales_{date.strftime(\"%Y-%m-%d\")}.xlsx'\n",
        "    df.to_excel(filename, index=False)\n",
        "    return filename\n",
        "\n",
        "# Create daily files for past week\n",
        "for i in range(7):\n",
        "    date = datetime.now().date() - timedelta(days=i)\n",
        "    file = create_daily_sales_file(date, output_dir)\n",
        "    logger.info(f\"Created: {file.name}\")\n",
        "\n",
        "print(f\"âœ“ Created 7 daily sales files\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. BATCH FILE PROCESSING\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"3. BATCH FILE PROCESSING\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "def process_all_files(directory, pattern='daily_sales_*.xlsx'):\n",
        "    \"\"\"Process all files matching pattern\"\"\"\n",
        "    logger.info(f\"Starting batch processing in {directory}\")\n",
        "    \n",
        "    files = list(Path(directory).glob(pattern))\n",
        "    logger.info(f\"Found {len(files)} files\")\n",
        "    \n",
        "    results = []\n",
        "    for file in sorted(files):\n",
        "        try:\n",
        "            logger.info(f\"Processing: {file.name}\")\n",
        "            df = pd.read_excel(file)\n",
        "            \n",
        "            summary = {\n",
        "                'File': file.name,\n",
        "                'Date': df['Date'].iloc[0],\n",
        "                'Rows': len(df),\n",
        "                'Total_Sales': df['Sales'].sum(),\n",
        "                'Total_Quantity': df['Quantity'].sum(),\n",
        "                'Status': 'Success'\n",
        "            }\n",
        "            results.append(summary)\n",
        "            logger.info(f\"  Success: {len(df)} rows, ${df['Sales'].sum():,} in sales\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"  Error with {file.name}: {e}\")\n",
        "            results.append({\n",
        "                'File': file.name,\n",
        "                'Status': f'Error: {e}'\n",
        "            })\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Process all daily files\n",
        "df_summary = process_all_files(output_dir)\n",
        "print(\"\\nProcessing Summary:\")\n",
        "print(df_summary.to_string(index=False))\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. AUTOMATED DAILY REPORT GENERATOR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"4. AUTOMATED DAILY REPORT GENERATOR\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "def generate_daily_report(data_dir, output_dir):\n",
        "    \"\"\"Generate daily report from latest data\"\"\"\n",
        "    timestamp = datetime.now().strftime('%Y-%m-%d')\n",
        "    logger.info(f\"Generating daily report for {timestamp}\")\n",
        "    \n",
        "    # Find latest file\n",
        "    files = sorted(Path(data_dir).glob('daily_sales_*.xlsx'))\n",
        "    if not files:\n",
        "        logger.warning(\"No files found\")\n",
        "        return None\n",
        "    \n",
        "    latest_file = files[-1]\n",
        "    logger.info(f\"Using latest file: {latest_file.name}\")\n",
        "    \n",
        "    # Read data\n",
        "    df = pd.read_excel(latest_file)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    metrics = {\n",
        "        'Report_Date': timestamp,\n",
        "        'Total_Sales': df['Sales'].sum(),\n",
        "        'Total_Orders': len(df),\n",
        "        'Average_Order': df['Sales'].mean(),\n",
        "        'Total_Quantity': df['Quantity'].sum(),\n",
        "        'Best_Product': df.groupby('Product')['Sales'].sum().idxmax(),\n",
        "        'Best_Region': df.groupby('Region')['Sales'].sum().idxmax()\n",
        "    }\n",
        "    \n",
        "    logger.info(f\"Metrics calculated: ${metrics['Total_Sales']:,} in sales\")\n",
        "    \n",
        "    # Create report\n",
        "    report_file = Path(output_dir) / f'daily_report_{timestamp}.xlsx'\n",
        "    \n",
        "    with pd.ExcelWriter(report_file) as writer:\n",
        "        # Summary metrics\n",
        "        df_metrics = pd.DataFrame([metrics])\n",
        "        df_metrics.to_excel(writer, sheet_name='Summary', index=False)\n",
        "        \n",
        "        # Raw data\n",
        "        df.to_excel(writer, sheet_name='Raw_Data', index=False)\n",
        "        \n",
        "        # By product\n",
        "        product_summary = df.groupby('Product').agg({\n",
        "            'Sales': 'sum',\n",
        "            'Quantity': 'sum'\n",
        "        }).reset_index()\n",
        "        product_summary.to_excel(writer, sheet_name='By_Product', index=False)\n",
        "        \n",
        "        # By region\n",
        "        region_summary = df.groupby('Region').agg({\n",
        "            'Sales': 'sum',\n",
        "            'Quantity': 'sum'\n",
        "        }).reset_index()\n",
        "        region_summary.to_excel(writer, sheet_name='By_Region', index=False)\n",
        "    \n",
        "    logger.info(f\"Report saved: {report_file.name}\")\n",
        "    print(f\"âœ“ Daily report generated: {report_file.name}\")\n",
        "    print()\n",
        "    \n",
        "    return report_file\n",
        "\n",
        "# Generate daily report\n",
        "daily_report = generate_daily_report(output_dir, reports_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. WEEKLY CONSOLIDATION\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"5. WEEKLY CONSOLIDATION\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "def weekly_consolidation(data_dir, output_dir):\n",
        "    \"\"\"Consolidate week's data into single report\"\"\"\n",
        "    logger.info(\"Starting weekly consolidation\")\n",
        "    \n",
        "    # Find all files\n",
        "    files = sorted(Path(data_dir).glob('daily_sales_*.xlsx'))\n",
        "    logger.info(f\"Found {len(files)} daily files\")\n",
        "    \n",
        "    # Read and combine all files\n",
        "    all_data = []\n",
        "    for file in files:\n",
        "        df = pd.read_excel(file)\n",
        "        df['Source_File'] = file.name\n",
        "        all_data.append(df)\n",
        "        logger.info(f\"  Read: {file.name}\")\n",
        "    \n",
        "    df_week = pd.concat(all_data, ignore_index=True)\n",
        "    logger.info(f\"Combined: {len(df_week)} total rows\")\n",
        "    \n",
        "    # Weekly analysis\n",
        "    week_number = datetime.now().isocalendar()[1]\n",
        "    \n",
        "    weekly_metrics = {\n",
        "        'Week_Number': week_number,\n",
        "        'Year': datetime.now().year,\n",
        "        'Total_Sales': df_week['Sales'].sum(),\n",
        "        'Total_Orders': len(df_week),\n",
        "        'Average_Daily_Sales': df_week.groupby('Date')['Sales'].sum().mean(),\n",
        "        'Best_Day': df_week.groupby('Date')['Sales'].sum().idxmax(),\n",
        "        'Days_Covered': df_week['Date'].nunique()\n",
        "    }\n",
        "    \n",
        "    # Save weekly report\n",
        "    week_file = Path(output_dir) / f'weekly_report_W{week_number}_{datetime.now().year}.xlsx'\n",
        "    \n",
        "    with pd.ExcelWriter(week_file) as writer:\n",
        "        # All data\n",
        "        df_week.to_excel(writer, sheet_name='All_Data', index=False)\n",
        "        \n",
        "        # Weekly metrics\n",
        "        df_metrics = pd.DataFrame([weekly_metrics])\n",
        "        df_metrics.to_excel(writer, sheet_name='Weekly_Summary', index=False)\n",
        "        \n",
        "        # Daily breakdown\n",
        "        daily_breakdown = df_week.groupby('Date').agg({\n",
        "            'Sales': 'sum',\n",
        "            'Quantity': 'sum'\n",
        "        }).reset_index()\n",
        "        daily_breakdown.to_excel(writer, sheet_name='Daily_Breakdown', index=False)\n",
        "        \n",
        "        # Product analysis\n",
        "        product_weekly = df_week.groupby('Product').agg({\n",
        "            'Sales': 'sum',\n",
        "            'Quantity': 'sum'\n",
        "        }).sort_values('Sales', ascending=False).reset_index()\n",
        "        product_weekly.to_excel(writer, sheet_name='Products', index=False)\n",
        "    \n",
        "    logger.info(f\"Weekly report saved: {week_file.name}\")\n",
        "    print(f\"âœ“ Weekly consolidation complete: {week_file.name}\")\n",
        "    print(f\"  Total sales: ${weekly_metrics['Total_Sales']:,}\")\n",
        "    print(f\"  Orders: {weekly_metrics['Total_Orders']:,}\")\n",
        "    print()\n",
        "    \n",
        "    return week_file\n",
        "\n",
        "# Generate weekly report\n",
        "weekly_report = weekly_consolidation(output_dir, reports_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. ERROR-SAFE FILE PROCESSOR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"6. ERROR-SAFE FILE PROCESSOR\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "def safe_file_processor(input_dir, output_file, pattern='*.xlsx'):\n",
        "    \"\"\"Process files with comprehensive error handling\"\"\"\n",
        "    logger.info(\"Starting safe file processor\")\n",
        "    \n",
        "    try:\n",
        "        files = list(Path(input_dir).glob(pattern))\n",
        "        if not files:\n",
        "            logger.warning(f\"No files found matching {pattern}\")\n",
        "            return False\n",
        "        \n",
        "        logger.info(f\"Found {len(files)} files\")\n",
        "        \n",
        "        results = []\n",
        "        successful = 0\n",
        "        failed = 0\n",
        "        \n",
        "        for file in files:\n",
        "            if file.name.startswith('~$'):  # Skip temp files\n",
        "                continue\n",
        "            \n",
        "            try:\n",
        "                logger.info(f\"Processing: {file.name}\")\n",
        "                \n",
        "                # Validate file\n",
        "                if file.stat().st_size == 0:\n",
        "                    raise ValueError(\"File is empty\")\n",
        "                \n",
        "                # Read\n",
        "                df = pd.read_excel(file)\n",
        "                \n",
        "                if df.empty:\n",
        "                    raise ValueError(\"DataFrame is empty\")\n",
        "                \n",
        "                # Process\n",
        "                results.append({\n",
        "                    'File': file.name,\n",
        "                    'Rows': len(df),\n",
        "                    'Columns': len(df.columns),\n",
        "                    'Status': 'Success',\n",
        "                    'Timestamp': datetime.now()\n",
        "                })\n",
        "                \n",
        "                successful += 1\n",
        "                logger.info(f\"  âœ“ Success: {len(df)} rows\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                logger.error(f\"  âœ— Error: {e}\")\n",
        "                results.append({\n",
        "                    'File': file.name,\n",
        "                    'Status': 'Failed',\n",
        "                    'Error': str(e),\n",
        "                    'Timestamp': datetime.now()\n",
        "                })\n",
        "                failed += 1\n",
        "        \n",
        "        # Save results\n",
        "        df_results = pd.DataFrame(results)\n",
        "        df_results.to_excel(output_file, index=False)\n",
        "        \n",
        "        logger.info(f\"Processing complete: {successful} successful, {failed} failed\")\n",
        "        logger.info(f\"Results saved: {output_file}\")\n",
        "        \n",
        "        print(f\"âœ“ Safe processing complete\")\n",
        "        print(f\"  Successful: {successful}\")\n",
        "        print(f\"  Failed: {failed}\")\n",
        "        print()\n",
        "        \n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Fatal error: {e}\")\n",
        "        return False\n",
        "\n",
        "# Run safe processor\n",
        "results_file = reports_dir / 'processing_results.xlsx'\n",
        "success = safe_file_processor(output_dir, results_file, 'daily_sales_*.xlsx')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. AUTOMATED CLEANUP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"7. AUTOMATED CLEANUP\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "def cleanup_old_files(directory, days_old=7, pattern='*.xlsx'):\n",
        "    \"\"\"Remove files older than specified days\"\"\"\n",
        "    logger.info(f\"Cleaning up files older than {days_old} days\")\n",
        "    \n",
        "    cutoff_date = datetime.now() - timedelta(days=days_old)\n",
        "    files = Path(directory).glob(pattern)\n",
        "    \n",
        "    removed_count = 0\n",
        "    for file in files:\n",
        "        file_time = datetime.fromtimestamp(file.stat().st_mtime)\n",
        "        if file_time < cutoff_date:\n",
        "            logger.info(f\"Removing old file: {file.name}\")\n",
        "            # file.unlink()  # Uncomment to actually delete\n",
        "            removed_count += 1\n",
        "            print(f\"  Would remove: {file.name} (from {file_time.date()})\")\n",
        "    \n",
        "    if removed_count == 0:\n",
        "        logger.info(\"No old files to remove\")\n",
        "        print(\"  No old files found\")\n",
        "    \n",
        "    print(f\"âœ“ Cleanup complete: {removed_count} files would be removed\")\n",
        "    print()\n",
        "    \n",
        "    return removed_count\n",
        "\n",
        "# Run cleanup (demonstration only, files not actually removed)\n",
        "cleanup_count = cleanup_old_files(output_dir, days_old=30)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. AUTOMATION FRAMEWORK CLASS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"8. AUTOMATION FRAMEWORK CLASS\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "class ExcelAutomationFramework:\n",
        "    \"\"\"Complete framework for Excel automation\"\"\"\n",
        "    \n",
        "    def __init__(self, input_dir, output_dir):\n",
        "        self.input_dir = Path(input_dir)\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.output_dir.mkdir(exist_ok=True)\n",
        "        \n",
        "        # Setup logging\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "        self.logger.info(\"Framework initialized\")\n",
        "    \n",
        "    def process_files(self, pattern='*.xlsx'):\n",
        "        \"\"\"Main processing workflow\"\"\"\n",
        "        self.logger.info(f\"Starting workflow for pattern: {pattern}\")\n",
        "        \n",
        "        files = list(self.input_dir.glob(pattern))\n",
        "        self.logger.info(f\"Found {len(files)} files\")\n",
        "        \n",
        "        if not files:\n",
        "            self.logger.warning(\"No files to process\")\n",
        "            return None\n",
        "        \n",
        "        # Process all files\n",
        "        all_data = []\n",
        "        for file in files:\n",
        "            df = self.process_single_file(file)\n",
        "            if df is not None:\n",
        "                all_data.append(df)\n",
        "        \n",
        "        # Combine results\n",
        "        if all_data:\n",
        "            df_final = pd.concat(all_data, ignore_index=True)\n",
        "            self.save_results(df_final)\n",
        "            return df_final\n",
        "        \n",
        "        return None\n",
        "    \n",
        "    def process_single_file(self, file_path):\n",
        "        \"\"\"Process individual file\"\"\"\n",
        "        try:\n",
        "            self.logger.info(f\"Processing: {file_path.name}\")\n",
        "            df = pd.read_excel(file_path)\n",
        "            \n",
        "            # Add metadata\n",
        "            df['Source_File'] = file_path.name\n",
        "            df['Processing_Date'] = datetime.now()\n",
        "            \n",
        "            return df\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error with {file_path.name}: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def save_results(self, df):\n",
        "        \"\"\"Save processed results\"\"\"\n",
        "        output_file = self.output_dir / f'combined_results_{datetime.now().strftime(\"%Y%m%d\")}.xlsx'\n",
        "        df.to_excel(output_file, index=False)\n",
        "        self.logger.info(f\"Results saved: {output_file.name}\")\n",
        "        print(f\"  âœ“ Saved: {output_file.name}\")\n",
        "\n",
        "# Use automation framework\n",
        "framework = ExcelAutomationFramework(output_dir, reports_dir)\n",
        "df_result = framework.process_files('daily_sales_*.xlsx')\n",
        "\n",
        "print(f\"âœ“ Framework processed {len(df_result) if df_result is not None else 0} total rows\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. FILE MONITORING SIMULATION\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"9. FILE MONITORING SIMULATION\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "def check_for_new_files(directory, processed_files=None):\n",
        "    \"\"\"Check for new files since last check\"\"\"\n",
        "    if processed_files is None:\n",
        "        processed_files = set()\n",
        "    \n",
        "    logger.info(\"Checking for new files\")\n",
        "    \n",
        "    current_files = set(f.name for f in Path(directory).glob('*.xlsx') if not f.name.startswith('~$'))\n",
        "    new_files = current_files - processed_files\n",
        "    \n",
        "    if new_files:\n",
        "        logger.info(f\"Found {len(new_files)} new files\")\n",
        "        for file in new_files:\n",
        "            print(f\"  New file detected: {file}\")\n",
        "    else:\n",
        "        logger.info(\"No new files\")\n",
        "        print(\"  No new files found\")\n",
        "    \n",
        "    print()\n",
        "    return current_files\n",
        "\n",
        "# Simulate file monitoring\n",
        "processed = check_for_new_files(output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. COMPREHENSIVE AUTOMATION WORKFLOW\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"10. COMPREHENSIVE AUTOMATION WORKFLOW\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "def run_automation_workflow():\n",
        "    \"\"\"Complete end-to-end automation workflow\"\"\"\n",
        "    logger.info(\"=\"*60)\n",
        "    logger.info(\"Starting comprehensive automation workflow\")\n",
        "    logger.info(\"=\"*60)\n",
        "    \n",
        "    workflow_steps = [\n",
        "        (\"Check for new files\", lambda: check_for_new_files(output_dir)),\n",
        "        (\"Process batch files\", lambda: process_all_files(output_dir)),\n",
        "        (\"Generate daily report\", lambda: generate_daily_report(output_dir, reports_dir)),\n",
        "        (\"Weekly consolidation\", lambda: weekly_consolidation(output_dir, reports_dir)),\n",
        "    ]\n",
        "    \n",
        "    results = {}\n",
        "    for step_name, step_func in workflow_steps:\n",
        "        try:\n",
        "            logger.info(f\"Step: {step_name}\")\n",
        "            result = step_func()\n",
        "            results[step_name] = \"Success\"\n",
        "            print(f\"âœ“ {step_name}: Complete\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Step failed: {step_name} - {e}\")\n",
        "            results[step_name] = f\"Failed: {e}\"\n",
        "            print(f\"âœ— {step_name}: Failed\")\n",
        "    \n",
        "    logger.info(\"Workflow complete\")\n",
        "    print(\"\\nWorkflow Summary:\")\n",
        "    for step, status in results.items():\n",
        "        print(f\"  {step}: {status}\")\n",
        "    print()\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run comprehensive workflow\n",
        "workflow_results = run_automation_workflow()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"SESSION 10 COMPLETE!\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "print(\"What you learned:\")\n",
        "print(\"âœ“ Setup logging for automation\")\n",
        "print(\"âœ“ Process files in batches\")\n",
        "print(\"âœ“ Generate automated daily reports\")\n",
        "print(\"âœ“ Create weekly consolidations\")\n",
        "print(\"âœ“ Handle errors gracefully\")\n",
        "print(\"âœ“ Build reusable automation frameworks\")\n",
        "print(\"âœ“ Monitor for new files\")\n",
        "print(\"âœ“ Run comprehensive workflows\")\n",
        "print(\"âœ“ Implement production-ready automation\")\n",
        "print()\n",
        "print(\"Files generated:\")\n",
        "for report_file in sorted(reports_dir.glob('*.xlsx')):\n",
        "    print(f\"  - {report_file.name}\")\n",
        "print()\n",
        "print(\"Log file:\")\n",
        "print(f\"  - {log_file.name}\")\n",
        "print()\n",
        "print(\"Next: Session 11 - Advanced Excel Features\")\n",
        "print(\"=\" * 80)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}