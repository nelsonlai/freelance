{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Session 4: Data Manipulation\n",
        "\n",
        "Welcome to this interactive session! This notebook contains all the examples from the Python script.\n",
        "\n",
        "## ðŸ“‹ How to Use This Notebook\n",
        "\n",
        "1. Run cells sequentially using Shift+Enter\n",
        "2. Modify the code and experiment\n",
        "3. Check the output after each cell\n",
        "4. Refer to the markdown guide for detailed explanations\n",
        "\n",
        "## ðŸš€ Let's Begin!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Session 4: Data Manipulation with Pandas - Comprehensive Examples\n",
        "=================================================================\n",
        "\n",
        "This script demonstrates comprehensive data manipulation techniques\n",
        "for cleaning, transforming, and analyzing Excel data.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Session 4: Data Manipulation with Pandas\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "# Create output directory\n",
        "output_dir = Path(__file__).parent / 'sample_files'\n",
        "output_dir.mkdir(exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. CREATE SAMPLE MESSY DATA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"1. CREATING SAMPLE MESSY DATA\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Create realistic messy dataset\n",
        "np.random.seed(42)\n",
        "\n",
        "messy_data = {\n",
        "    'Employee_ID': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 101],  # Duplicate\n",
        "    'Name': ['Alice Smith', 'bob jones', '  Charlie  ', 'Diana Lee', None, \n",
        "             'Eve Wilson', 'Frank Brown', 'Grace Kim', 'Henry Davis', 'Ivy Chen', 'Alice Smith'],\n",
        "    'Age': [25, 30, None, 28, 32, 27, 35, np.nan, 29, 31, 25],\n",
        "    'Department': ['Sales', 'IT', 'HR', 'Sales', 'IT', 'HR', None, 'Sales', 'IT', 'HR', 'Sales'],\n",
        "    'Salary': [60000, 75000, 58000, np.nan, 72000, 55000, 80000, 65000, 70000, 60000, 60000],\n",
        "    'Join_Date': ['2020-01-15', '2019-03-20', '2021-06-10', '2020-08-05', '2018-11-22',\n",
        "                  '2021-02-14', '2017-09-30', '2020-05-18', '2019-07-25', '2021-01-08', '2020-01-15']\n",
        "}\n",
        "\n",
        "df_messy = pd.DataFrame(messy_data)\n",
        "messy_file = output_dir / 'messy_employee_data.xlsx'\n",
        "df_messy.to_excel(messy_file, index=False)\n",
        "\n",
        "print(\"Created messy dataset:\")\n",
        "print(df_messy)\n",
        "print(f\"\\nâœ“ Saved to: {messy_file}\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. DATA INSPECTION\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"2. DATA INSPECTION\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "print(\"Basic Information:\")\n",
        "print(df_messy.info())\n",
        "print()\n",
        "\n",
        "print(\"First 5 rows:\")\n",
        "print(df_messy.head())\n",
        "print()\n",
        "\n",
        "print(\"Statistical Summary:\")\n",
        "print(df_messy.describe())\n",
        "print()\n",
        "\n",
        "print(\"Data Types:\")\n",
        "print(df_messy.dtypes)\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. HANDLING MISSING VALUES\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"3. HANDLING MISSING VALUES\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "print(\"Missing values per column:\")\n",
        "print(df_messy.isnull().sum())\n",
        "print()\n",
        "\n",
        "print(\"Total missing values:\", df_messy.isnull().sum().sum())\n",
        "print()\n",
        "\n",
        "# Show rows with missing values\n",
        "print(\"Rows with missing values:\")\n",
        "print(df_messy[df_messy.isnull().any(axis=1)])\n",
        "print()\n",
        "\n",
        "# Different strategies for handling missing values\n",
        "df_clean = df_messy.copy()\n",
        "\n",
        "# Fill numeric columns with mean\n",
        "df_clean['Age'] = df_clean['Age'].fillna(df_clean['Age'].mean())\n",
        "df_clean['Salary'] = df_clean['Salary'].fillna(df_clean['Salary'].median())\n",
        "\n",
        "# Fill categorical columns with mode or 'Unknown'\n",
        "df_clean['Name'] = df_clean['Name'].fillna('Unknown')\n",
        "df_clean['Department'] = df_clean['Department'].fillna('Unknown')\n",
        "\n",
        "print(\"After filling missing values:\")\n",
        "print(df_clean.isnull().sum())\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. REMOVING DUPLICATES\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"4. REMOVING DUPLICATES\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "print(f\"Original rows: {len(df_clean)}\")\n",
        "print(f\"Duplicate rows: {df_clean.duplicated().sum()}\")\n",
        "print()\n",
        "\n",
        "print(\"Duplicate rows:\")\n",
        "print(df_clean[df_clean.duplicated(keep=False)])\n",
        "print()\n",
        "\n",
        "# Remove duplicates\n",
        "df_clean = df_clean.drop_duplicates()\n",
        "print(f\"Rows after removing duplicates: {len(df_clean)}\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. STRING CLEANING\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"5. STRING CLEANING\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "print(\"Before cleaning:\")\n",
        "print(df_clean[['Name', 'Department']].head())\n",
        "print()\n",
        "\n",
        "# Clean string columns\n",
        "df_clean['Name'] = df_clean['Name'].str.strip()  # Remove whitespace\n",
        "df_clean['Name'] = df_clean['Name'].str.title()  # Proper case\n",
        "df_clean['Department'] = df_clean['Department'].str.upper()  # Uppercase\n",
        "\n",
        "print(\"After cleaning:\")\n",
        "print(df_clean[['Name', 'Department']].head())\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. DATA TYPE CONVERSION\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"6. DATA TYPE CONVERSION\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "print(\"Before conversion:\")\n",
        "print(df_clean.dtypes)\n",
        "print()\n",
        "\n",
        "# Convert Join_Date to datetime\n",
        "df_clean['Join_Date'] = pd.to_datetime(df_clean['Join_Date'])\n",
        "\n",
        "# Ensure numeric types\n",
        "df_clean['Age'] = df_clean['Age'].astype(int)\n",
        "df_clean['Salary'] = df_clean['Salary'].astype(float)\n",
        "\n",
        "print(\"After conversion:\")\n",
        "print(df_clean.dtypes)\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. FILTERING DATA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"7. FILTERING DATA\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Single condition\n",
        "high_earners = df_clean[df_clean['Salary'] > 65000]\n",
        "print(f\"Employees earning > $65,000: {len(high_earners)}\")\n",
        "print(high_earners[['Name', 'Salary']])\n",
        "print()\n",
        "\n",
        "# Multiple conditions (AND)\n",
        "young_high_earners = df_clean[(df_clean['Age'] < 30) & (df_clean['Salary'] > 65000)]\n",
        "print(f\"Young high earners (Age < 30 AND Salary > 65000): {len(young_high_earners)}\")\n",
        "print(young_high_earners[['Name', 'Age', 'Salary']])\n",
        "print()\n",
        "\n",
        "# Multiple conditions (OR)\n",
        "sales_or_it = df_clean[(df_clean['Department'] == 'SALES') | (df_clean['Department'] == 'IT')]\n",
        "print(f\"Sales or IT employees: {len(sales_or_it)}\")\n",
        "print(sales_or_it[['Name', 'Department']])\n",
        "print()\n",
        "\n",
        "# Using isin\n",
        "selected_depts = df_clean[df_clean['Department'].isin(['SALES', 'IT'])]\n",
        "print(f\"Selected departments (using isin): {len(selected_depts)}\")\n",
        "print()\n",
        "\n",
        "# String contains\n",
        "names_with_n = df_clean[df_clean['Name'].str.contains('n', case=False, na=False)]\n",
        "print(f\"Names containing 'n': {len(names_with_n)}\")\n",
        "print(names_with_n[['Name']])\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. SORTING DATA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"8. SORTING DATA\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Sort by single column\n",
        "sorted_by_age = df_clean.sort_values('Age')\n",
        "print(\"Sorted by Age (ascending):\")\n",
        "print(sorted_by_age[['Name', 'Age']].head())\n",
        "print()\n",
        "\n",
        "# Sort descending\n",
        "sorted_by_salary = df_clean.sort_values('Salary', ascending=False)\n",
        "print(\"Sorted by Salary (descending):\")\n",
        "print(sorted_by_salary[['Name', 'Salary']].head())\n",
        "print()\n",
        "\n",
        "# Sort by multiple columns\n",
        "sorted_multi = df_clean.sort_values(['Department', 'Age'])\n",
        "print(\"Sorted by Department, then Age:\")\n",
        "print(sorted_multi[['Name', 'Department', 'Age']])\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. GROUPING AND AGGREGATION\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"9. GROUPING AND AGGREGATION\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Simple groupby\n",
        "dept_counts = df_clean.groupby('Department').size()\n",
        "print(\"Employees per department:\")\n",
        "print(dept_counts)\n",
        "print()\n",
        "\n",
        "# Aggregate specific column\n",
        "avg_salary_by_dept = df_clean.groupby('Department')['Salary'].mean().round(2)\n",
        "print(\"Average salary by department:\")\n",
        "print(avg_salary_by_dept)\n",
        "print()\n",
        "\n",
        "# Multiple aggregations\n",
        "dept_summary = df_clean.groupby('Department').agg({\n",
        "    'Salary': ['mean', 'min', 'max', 'count'],\n",
        "    'Age': ['mean', 'min', 'max']\n",
        "}).round(2)\n",
        "print(\"Department summary:\")\n",
        "print(dept_summary)\n",
        "print()\n",
        "\n",
        "# Named aggregations\n",
        "dept_stats = df_clean.groupby('Department').agg(\n",
        "    avg_salary=('Salary', 'mean'),\n",
        "    total_employees=('Employee_ID', 'count'),\n",
        "    avg_age=('Age', 'mean')\n",
        ").round(2)\n",
        "print(\"Department statistics (named):\")\n",
        "print(dept_stats)\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. CREATING CALCULATED COLUMNS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"10. CREATING CALCULATED COLUMNS\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Simple calculation\n",
        "df_clean['Salary_K'] = (df_clean['Salary'] / 1000).round(1)\n",
        "print(\"Added Salary_K column:\")\n",
        "print(df_clean[['Name', 'Salary', 'Salary_K']].head())\n",
        "print()\n",
        "\n",
        "# Conditional calculation\n",
        "df_clean['Bonus'] = 0\n",
        "df_clean.loc[df_clean['Department'] == 'SALES', 'Bonus'] = df_clean['Salary'] * 0.15\n",
        "df_clean.loc[df_clean['Department'] == 'IT', 'Bonus'] = df_clean['Salary'] * 0.10\n",
        "df_clean.loc[df_clean['Department'] == 'HR', 'Bonus'] = df_clean['Salary'] * 0.08\n",
        "\n",
        "print(\"Added Bonus column:\")\n",
        "print(df_clean[['Name', 'Department', 'Salary', 'Bonus']].head())\n",
        "print()\n",
        "\n",
        "# Using apply with lambda\n",
        "df_clean['Tax'] = df_clean['Salary'].apply(lambda x: x * 0.25)\n",
        "print(\"Added Tax column:\")\n",
        "print(df_clean[['Name', 'Salary', 'Tax']].head())\n",
        "print()\n",
        "\n",
        "# Using apply with function\n",
        "def categorize_age(age):\n",
        "    if age < 25:\n",
        "        return 'Junior'\n",
        "    elif age < 35:\n",
        "        return 'Mid'\n",
        "    else:\n",
        "        return 'Senior'\n",
        "\n",
        "df_clean['Seniority'] = df_clean['Age'].apply(categorize_age)\n",
        "print(\"Added Seniority column:\")\n",
        "print(df_clean[['Name', 'Age', 'Seniority']])\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. BINNING AND CATEGORIZING\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"11. BINNING AND CATEGORIZING\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Create age groups\n",
        "df_clean['Age_Group'] = pd.cut(\n",
        "    df_clean['Age'],\n",
        "    bins=[0, 28, 32, 100],\n",
        "    labels=['Young', 'Middle', 'Senior']\n",
        ")\n",
        "print(\"Age groups:\")\n",
        "print(df_clean[['Name', 'Age', 'Age_Group']])\n",
        "print()\n",
        "\n",
        "# Quantile-based binning\n",
        "df_clean['Salary_Quartile'] = pd.qcut(\n",
        "    df_clean['Salary'],\n",
        "    q=4,\n",
        "    labels=['Q1', 'Q2', 'Q3', 'Q4']\n",
        ")\n",
        "print(\"Salary quartiles:\")\n",
        "print(df_clean[['Name', 'Salary', 'Salary_Quartile']])\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. DATE OPERATIONS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"12. DATE OPERATIONS\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Extract date components\n",
        "df_clean['Join_Year'] = df_clean['Join_Date'].dt.year\n",
        "df_clean['Join_Month'] = df_clean['Join_Date'].dt.month\n",
        "df_clean['Join_Day_Of_Week'] = df_clean['Join_Date'].dt.day_name()\n",
        "\n",
        "print(\"Date components:\")\n",
        "print(df_clean[['Name', 'Join_Date', 'Join_Year', 'Join_Month', 'Join_Day_Of_Week']])\n",
        "print()\n",
        "\n",
        "# Calculate tenure\n",
        "today = pd.Timestamp.now()\n",
        "df_clean['Tenure_Days'] = (today - df_clean['Join_Date']).dt.days\n",
        "df_clean['Tenure_Years'] = (df_clean['Tenure_Days'] / 365).round(1)\n",
        "\n",
        "print(\"Tenure calculation:\")\n",
        "print(df_clean[['Name', 'Join_Date', 'Tenure_Years']])\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. PIVOT TABLES\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"13. PIVOT TABLES\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Simple pivot table\n",
        "pivot_salary = pd.pivot_table(\n",
        "    df_clean,\n",
        "    values='Salary',\n",
        "    index='Department',\n",
        "    aggfunc='mean'\n",
        ").round(2)\n",
        "print(\"Average salary by department (pivot):\")\n",
        "print(pivot_salary)\n",
        "print()\n",
        "\n",
        "# Multi-level pivot\n",
        "pivot_multi = pd.pivot_table(\n",
        "    df_clean,\n",
        "    values='Salary',\n",
        "    index='Department',\n",
        "    columns='Seniority',\n",
        "    aggfunc='mean',\n",
        "    fill_value=0\n",
        ").round(2)\n",
        "print(\"Average salary by department and seniority:\")\n",
        "print(pivot_multi)\n",
        "print()\n",
        "\n",
        "# Multiple aggregations in pivot\n",
        "pivot_detailed = pd.pivot_table(\n",
        "    df_clean,\n",
        "    values='Salary',\n",
        "    index='Department',\n",
        "    aggfunc=['mean', 'count', 'sum'],\n",
        "    margins=True  # Add totals\n",
        ").round(2)\n",
        "print(\"Detailed salary pivot with totals:\")\n",
        "print(pivot_detailed)\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. CROSS-TABULATION\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"14. CROSS-TABULATION\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Simple crosstab\n",
        "crosstab = pd.crosstab(df_clean['Department'], df_clean['Seniority'])\n",
        "print(\"Employee count by department and seniority:\")\n",
        "print(crosstab)\n",
        "print()\n",
        "\n",
        "# Crosstab with percentages\n",
        "crosstab_pct = pd.crosstab(\n",
        "    df_clean['Department'],\n",
        "    df_clean['Seniority'],\n",
        "    normalize='index'\n",
        ") * 100\n",
        "print(\"Percentage distribution by department:\")\n",
        "print(crosstab_pct.round(1))\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. REAL-WORLD EXAMPLE: SALES DATA ANALYSIS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"15. REAL-WORLD EXAMPLE: SALES DATA ANALYSIS\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Create comprehensive sales dataset\n",
        "sales_data = {\n",
        "    'Date': pd.date_range('2024-01-01', periods=100, freq='D'),\n",
        "    'Region': np.random.choice(['North', 'South', 'East', 'West'], 100),\n",
        "    'Product': np.random.choice(['Laptop', 'Phone', 'Tablet', 'Monitor'], 100),\n",
        "    'Quantity': np.random.randint(1, 20, 100),\n",
        "    'Price': np.random.choice([999, 599, 399, 299], 100),\n",
        "    'Salesperson': np.random.choice(['Alice', 'Bob', 'Charlie', 'Diana'], 100)\n",
        "}\n",
        "\n",
        "df_sales = pd.DataFrame(sales_data)\n",
        "\n",
        "# Calculate revenue\n",
        "df_sales['Revenue'] = df_sales['Quantity'] * df_sales['Price']\n",
        "\n",
        "# Add time-based columns\n",
        "df_sales['Month'] = df_sales['Date'].dt.month\n",
        "df_sales['Week'] = df_sales['Date'].dt.isocalendar().week\n",
        "df_sales['Day_Of_Week'] = df_sales['Date'].dt.day_name()\n",
        "\n",
        "print(\"Sales data preview:\")\n",
        "print(df_sales.head(10))\n",
        "print()\n",
        "\n",
        "# Analysis 1: Total revenue by region\n",
        "regional_revenue = df_sales.groupby('Region')['Revenue'].sum().sort_values(ascending=False)\n",
        "print(\"Total revenue by region:\")\n",
        "print(regional_revenue)\n",
        "print()\n",
        "\n",
        "# Analysis 2: Top products\n",
        "product_performance = df_sales.groupby('Product').agg({\n",
        "    'Revenue': 'sum',\n",
        "    'Quantity': 'sum'\n",
        "}).sort_values('Revenue', ascending=False)\n",
        "print(\"Product performance:\")\n",
        "print(product_performance)\n",
        "print()\n",
        "\n",
        "# Analysis 3: Best salespeople\n",
        "salesperson_stats = df_sales.groupby('Salesperson').agg({\n",
        "    'Revenue': 'sum',\n",
        "    'Quantity': 'sum',\n",
        "    'Date': 'count'\n",
        "}).rename(columns={'Date': 'Transactions'}).sort_values('Revenue', ascending=False)\n",
        "print(\"Salesperson performance:\")\n",
        "print(salesperson_stats)\n",
        "print()\n",
        "\n",
        "# Analysis 4: Monthly trends\n",
        "monthly_trend = df_sales.groupby('Month').agg({\n",
        "    'Revenue': 'sum',\n",
        "    'Quantity': 'sum'\n",
        "})\n",
        "print(\"Monthly sales trend:\")\n",
        "print(monthly_trend)\n",
        "print()\n",
        "\n",
        "# Analysis 5: Regional product preferences\n",
        "regional_product_pivot = pd.pivot_table(\n",
        "    df_sales,\n",
        "    values='Revenue',\n",
        "    index='Region',\n",
        "    columns='Product',\n",
        "    aggfunc='sum',\n",
        "    fill_value=0\n",
        ")\n",
        "print(\"Revenue by region and product:\")\n",
        "print(regional_product_pivot)\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 16. EXPORT CLEANED DATA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"16. EXPORT CLEANED DATA\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Save cleaned employee data\n",
        "clean_employee_file = output_dir / 'clean_employee_data.xlsx'\n",
        "df_clean.to_excel(clean_employee_file, index=False)\n",
        "print(f\"âœ“ Cleaned employee data saved to: {clean_employee_file}\")\n",
        "\n",
        "# Save sales analysis\n",
        "sales_analysis_file = output_dir / 'sales_analysis.xlsx'\n",
        "with pd.ExcelWriter(sales_analysis_file) as writer:\n",
        "    df_sales.to_excel(writer, sheet_name='Raw_Data', index=False)\n",
        "    regional_revenue.to_excel(writer, sheet_name='Regional_Revenue')\n",
        "    product_performance.to_excel(writer, sheet_name='Product_Performance')\n",
        "    salesperson_stats.to_excel(writer, sheet_name='Salesperson_Stats')\n",
        "    monthly_trend.to_excel(writer, sheet_name='Monthly_Trend')\n",
        "    regional_product_pivot.to_excel(writer, sheet_name='Regional_Product')\n",
        "\n",
        "print(f\"âœ“ Sales analysis saved to: {sales_analysis_file}\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"SESSION 4 COMPLETE!\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "print(\"What you learned:\")\n",
        "print(\"âœ“ Inspect and understand data\")\n",
        "print(\"âœ“ Handle missing values effectively\")\n",
        "print(\"âœ“ Remove duplicates\")\n",
        "print(\"âœ“ Clean string data\")\n",
        "print(\"âœ“ Convert data types\")\n",
        "print(\"âœ“ Filter and select data\")\n",
        "print(\"âœ“ Sort data by multiple columns\")\n",
        "print(\"âœ“ Group and aggregate data\")\n",
        "print(\"âœ“ Create calculated columns\")\n",
        "print(\"âœ“ Bin and categorize values\")\n",
        "print(\"âœ“ Work with dates\")\n",
        "print(\"âœ“ Create pivot tables\")\n",
        "print(\"âœ“ Perform cross-tabulation\")\n",
        "print(\"âœ“ Analyze real-world sales data\")\n",
        "print()\n",
        "print(\"Files created:\")\n",
        "for file in sorted(output_dir.glob('*.xlsx')):\n",
        "    print(f\"  - {file.name}\")\n",
        "print()\n",
        "print(\"Next: Session 5 - Excel Formatting and Styling\")\n",
        "print(\"=\" * 80)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}