{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Session 9: Working with Large Files\n",
        "\n",
        "Welcome to this interactive session! This notebook contains all the examples from the Python script.\n",
        "\n",
        "## ðŸ“‹ How to Use This Notebook\n",
        "\n",
        "1. Run cells sequentially using Shift+Enter\n",
        "2. Modify the code and experiment\n",
        "3. Check the output after each cell\n",
        "4. Refer to the markdown guide for detailed explanations\n",
        "\n",
        "## ðŸš€ Let's Begin!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Session 9: Working with Large Excel Files - Comprehensive Examples\n",
        "==================================================================\n",
        "\n",
        "This script demonstrates techniques for efficiently handling large Excel files\n",
        "including chunking, streaming, and optimization strategies.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from openpyxl import Workbook, load_workbook\n",
        "import time\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Session 9: Working with Large Excel Files\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "# Create output directory\n",
        "output_dir = Path(__file__).parent / 'sample_files'\n",
        "output_dir.mkdir(exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. CREATE SAMPLE LARGE FILES\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"1. CREATING SAMPLE LARGE FILES\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "def create_sample_file(filename, num_rows=10000):\n",
        "    \"\"\"Create sample file with specified number of rows\"\"\"\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    df = pd.DataFrame({\n",
        "        'ID': range(1, num_rows + 1),\n",
        "        'Product': np.random.choice(['Laptop', 'Mouse', 'Keyboard', 'Monitor'], num_rows),\n",
        "        'Region': np.random.choice(['North', 'South', 'East', 'West'], num_rows),\n",
        "        'Sales': np.random.randint(100, 10000, num_rows),\n",
        "        'Quantity': np.random.randint(1, 100, num_rows),\n",
        "        'Date': pd.date_range('2024-01-01', periods=num_rows, freq='H')\n",
        "    })\n",
        "    \n",
        "    df.to_excel(filename, index=False)\n",
        "    file_size = Path(filename).stat().st_size / (1024 * 1024)  # MB\n",
        "    return len(df), file_size\n",
        "\n",
        "# Create small file\n",
        "small_file = output_dir / 'small_file.xlsx'\n",
        "rows, size = create_sample_file(small_file, 1000)\n",
        "print(f\"âœ“ Created {small_file.name}: {rows:,} rows, {size:.2f} MB\")\n",
        "\n",
        "# Create medium file\n",
        "medium_file = output_dir / 'medium_file.xlsx'\n",
        "rows, size = create_sample_file(medium_file, 5000)\n",
        "print(f\"âœ“ Created {medium_file.name}: {rows:,} rows, {size:.2f} MB\")\n",
        "\n",
        "# Create large file\n",
        "large_file = output_dir / 'large_file.xlsx'\n",
        "rows, size = create_sample_file(large_file, 10000)\n",
        "print(f\"âœ“ Created {large_file.name}: {rows:,} rows, {size:.2f} MB\")\n",
        "\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. MEASURE FILE SIZE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"2. MEASURING FILE SIZE\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "def check_file_size(file_path):\n",
        "    \"\"\"Check file size and categorize\"\"\"\n",
        "    path = Path(file_path)\n",
        "    size_bytes = path.stat().st_size\n",
        "    size_mb = size_bytes / (1024 * 1024)\n",
        "    \n",
        "    print(f\"File: {path.name}\")\n",
        "    print(f\"Size: {size_mb:.2f} MB\")\n",
        "    \n",
        "    if size_mb < 10:\n",
        "        category = \"Small - normal processing OK\"\n",
        "    elif size_mb < 50:\n",
        "        category = \"Medium - consider chunking\"\n",
        "    elif size_mb < 200:\n",
        "        category = \"Large - use chunking/streaming\"\n",
        "    else:\n",
        "        category = \"Very large - use optimized strategies\"\n",
        "    \n",
        "    print(f\"Category: {category}\")\n",
        "    print()\n",
        "    return size_mb\n",
        "\n",
        "check_file_size(small_file)\n",
        "check_file_size(medium_file)\n",
        "check_file_size(large_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. READING WITH PERFORMANCE MEASUREMENT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"3. READING WITH PERFORMANCE MEASUREMENT\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "def timed_read(file_path, method=\"normal\"):\n",
        "    \"\"\"Read file and measure time\"\"\"\n",
        "    start_time = time.time()\n",
        "    \n",
        "    if method == \"normal\":\n",
        "        df = pd.read_excel(file_path)\n",
        "    elif method == \"limited_cols\":\n",
        "        df = pd.read_excel(file_path, usecols=['ID', 'Product', 'Sales'])\n",
        "    elif method == \"limited_rows\":\n",
        "        df = pd.read_excel(file_path, nrows=1000)\n",
        "    \n",
        "    elapsed = time.time() - start_time\n",
        "    \n",
        "    return df, elapsed\n",
        "\n",
        "# Test different reading methods\n",
        "print(f\"Reading {small_file.name}:\")\n",
        "df1, time1 = timed_read(small_file, \"normal\")\n",
        "print(f\"  Normal read: {time1:.2f} seconds, {len(df1):,} rows\")\n",
        "\n",
        "df2, time2 = timed_read(small_file, \"limited_cols\")\n",
        "print(f\"  Limited columns: {time2:.2f} seconds, {len(df2.columns)} cols\")\n",
        "\n",
        "df3, time3 = timed_read(small_file, \"limited_rows\")\n",
        "print(f\"  Limited rows: {time3:.2f} seconds, {len(df3):,} rows\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. CHUNKED READING\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"4. CHUNKED READING\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "def read_in_chunks(file_path, chunk_size=2000):\n",
        "    \"\"\"Read large file in chunks and process\"\"\"\n",
        "    print(f\"Reading {Path(file_path).name} in chunks of {chunk_size}\")\n",
        "    \n",
        "    results = []\n",
        "    skip_rows = 0\n",
        "    chunk_num = 0\n",
        "    \n",
        "    while True:\n",
        "        try:\n",
        "            if skip_rows > 0:\n",
        "                df_chunk = pd.read_excel(\n",
        "                    file_path,\n",
        "                    skiprows=range(1, skip_rows + 1),\n",
        "                    nrows=chunk_size\n",
        "                )\n",
        "            else:\n",
        "                df_chunk = pd.read_excel(file_path, nrows=chunk_size)\n",
        "            \n",
        "            if df_chunk.empty:\n",
        "                break\n",
        "            \n",
        "            chunk_num += 1\n",
        "            \n",
        "            # Process chunk (example: calculate sum)\n",
        "            chunk_result = df_chunk['Sales'].sum()\n",
        "            results.append(chunk_result)\n",
        "            \n",
        "            print(f\"  Chunk {chunk_num}: {len(df_chunk):,} rows, Sales sum: ${chunk_result:,}\")\n",
        "            \n",
        "            skip_rows += chunk_size\n",
        "            \n",
        "            if len(df_chunk) < chunk_size:\n",
        "                break\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"  Error: {e}\")\n",
        "            break\n",
        "    \n",
        "    total = sum(results)\n",
        "    print(f\"  Total from {chunk_num} chunks: ${total:,}\")\n",
        "    print()\n",
        "    return total\n",
        "\n",
        "# Read large file in chunks\n",
        "total_sales = read_in_chunks(large_file, chunk_size=2500)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. STREAMING WITH OPENPYXL (READ-ONLY MODE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"5. STREAMING WITH OPENPYXL (READ-ONLY MODE)\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "def stream_read(file_path):\n",
        "    \"\"\"Read file using openpyxl streaming (memory efficient)\"\"\"\n",
        "    print(f\"Streaming {Path(file_path).name} with openpyxl...\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    wb = load_workbook(file_path, read_only=True)\n",
        "    ws = wb.active\n",
        "    \n",
        "    row_count = 0\n",
        "    total_sales = 0\n",
        "    \n",
        "    # Skip header\n",
        "    for i, row in enumerate(ws.iter_rows(min_row=2, values_only=True)):\n",
        "        row_count += 1\n",
        "        if row[3] is not None:  # Sales column\n",
        "            total_sales += row[3]\n",
        "        \n",
        "        if row_count % 2000 == 0:\n",
        "            print(f\"  Processed {row_count:,} rows...\")\n",
        "    \n",
        "    wb.close()\n",
        "    \n",
        "    elapsed = time.time() - start_time\n",
        "    \n",
        "    print(f\"  Completed: {row_count:,} rows in {elapsed:.2f} seconds\")\n",
        "    print(f\"  Total sales: ${total_sales:,}\")\n",
        "    print()\n",
        "    \n",
        "    return row_count, total_sales\n",
        "\n",
        "# Stream read large file\n",
        "rows, sales = stream_read(large_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. WRITE-ONLY MODE (MEMORY EFFICIENT WRITING)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"6. WRITE-ONLY MODE (MEMORY EFFICIENT WRITING)\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "def write_large_file(output_file, num_rows=10000):\n",
        "    \"\"\"Write large file using write-only mode\"\"\"\n",
        "    print(f\"Writing {num_rows:,} rows in write-only mode...\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    wb = Workbook(write_only=True)\n",
        "    ws = wb.create_sheet()\n",
        "    \n",
        "    # Write header\n",
        "    ws.append(['ID', 'Value', 'Category', 'Amount'])\n",
        "    \n",
        "    # Write data\n",
        "    for i in range(1, num_rows + 1):\n",
        "        ws.append([\n",
        "            i,\n",
        "            np.random.randint(100, 1000),\n",
        "            np.random.choice(['A', 'B', 'C']),\n",
        "            np.random.uniform(10.0, 1000.0)\n",
        "        ])\n",
        "        \n",
        "        if i % 2000 == 0:\n",
        "            print(f\"  Wrote {i:,} rows...\")\n",
        "    \n",
        "    wb.save(output_file)\n",
        "    \n",
        "    elapsed = time.time() - start_time\n",
        "    file_size = Path(output_file).stat().st_size / (1024 * 1024)\n",
        "    \n",
        "    print(f\"  Completed in {elapsed:.2f} seconds\")\n",
        "    print(f\"  File size: {file_size:.2f} MB\")\n",
        "    print()\n",
        "\n",
        "write_only_file = output_dir / 'write_only_large.xlsx'\n",
        "write_large_file(write_only_file, 10000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. OPTIMIZING DATA TYPES\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"7. OPTIMIZING DATA TYPES\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Read with default types\n",
        "df_default = pd.read_excel(small_file)\n",
        "memory_default = df_default.memory_usage(deep=True).sum() / (1024 * 1024)\n",
        "print(f\"Default types memory: {memory_default:.2f} MB\")\n",
        "print(df_default.dtypes)\n",
        "print()\n",
        "\n",
        "# Read and optimize types\n",
        "df_optimized = pd.read_excel(small_file)\n",
        "df_optimized['ID'] = df_optimized['ID'].astype('int32')\n",
        "df_optimized['Product'] = df_optimized['Product'].astype('category')\n",
        "df_optimized['Region'] = df_optimized['Region'].astype('category')\n",
        "df_optimized['Sales'] = df_optimized['Sales'].astype('int32')\n",
        "df_optimized['Quantity'] = df_optimized['Quantity'].astype('int16')\n",
        "\n",
        "memory_optimized = df_optimized.memory_usage(deep=True).sum() / (1024 * 1024)\n",
        "print(f\"Optimized types memory: {memory_optimized:.2f} MB\")\n",
        "print(df_optimized.dtypes)\n",
        "print(f\"Memory saved: {memory_default - memory_optimized:.2f} MB ({((memory_default - memory_optimized) / memory_default * 100):.1f}%)\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. READING ONLY NEEDED COLUMNS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"8. READING ONLY NEEDED COLUMNS\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Time full read\n",
        "start = time.time()\n",
        "df_full = pd.read_excel(small_file)\n",
        "time_full = time.time() - start\n",
        "\n",
        "# Time partial read\n",
        "start = time.time()\n",
        "df_partial = pd.read_excel(small_file, usecols=['Product', 'Sales', 'Region'])\n",
        "time_partial = time.time() - start\n",
        "\n",
        "print(f\"Full read: {len(df_full.columns)} columns, {time_full:.3f} seconds\")\n",
        "print(f\"Partial read: {len(df_partial.columns)} columns, {time_partial:.3f} seconds\")\n",
        "print(f\"Time saved: {time_full - time_partial:.3f} seconds ({((time_full - time_partial) / time_full * 100):.1f}%)\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. CSV CONVERSION FOR SPEED\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"9. CSV CONVERSION FOR SPEED\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Excel read\n",
        "start = time.time()\n",
        "df_excel = pd.read_excel(small_file)\n",
        "time_excel = time.time() - start\n",
        "\n",
        "# Convert to CSV\n",
        "csv_file = output_dir / 'converted.csv'\n",
        "df_excel.to_csv(csv_file, index=False)\n",
        "\n",
        "# CSV read\n",
        "start = time.time()\n",
        "df_csv = pd.read_csv(csv_file)\n",
        "time_csv = time.time() - start\n",
        "\n",
        "print(f\"Excel read: {time_excel:.3f} seconds\")\n",
        "print(f\"CSV read: {time_csv:.3f} seconds\")\n",
        "print(f\"Speedup: {time_excel / time_csv:.1f}x faster\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. BATCH PROCESSING MULTIPLE FILES\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"10. BATCH PROCESSING MULTIPLE FILES\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "def process_multiple_files(directory, pattern='*.xlsx'):\n",
        "    \"\"\"Process all files in directory\"\"\"\n",
        "    files = list(Path(directory).glob(pattern))\n",
        "    \n",
        "    print(f\"Found {len(files)} files to process\")\n",
        "    \n",
        "    results = []\n",
        "    for file in files:\n",
        "        if file.name.startswith('~$'):  # Skip temp files\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            df = pd.read_excel(file)\n",
        "            result = {\n",
        "                'File': file.name,\n",
        "                'Rows': len(df),\n",
        "                'Columns': len(df.columns),\n",
        "                'Size_MB': file.stat().st_size / (1024 * 1024)\n",
        "            }\n",
        "            results.append(result)\n",
        "            print(f\"  âœ“ {file.name}: {len(df):,} rows\")\n",
        "        except Exception as e:\n",
        "            print(f\"  âœ— {file.name}: {e}\")\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "summary = process_multiple_files(output_dir)\n",
        "print(\"\\nSummary:\")\n",
        "print(summary.to_string(index=False))\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. PRODUCTION-READY LARGE FILE PROCESSOR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"11. PRODUCTION-READY LARGE FILE PROCESSOR\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "def production_file_processor(input_file, output_file, chunk_size=5000):\n",
        "    \"\"\"\n",
        "    Production-ready processor with error handling and progress\n",
        "    \"\"\"\n",
        "    print(f\"Processing: {Path(input_file).name}\")\n",
        "    \n",
        "    try:\n",
        "        # Check file size\n",
        "        file_size_mb = Path(input_file).stat().st_size / (1024 * 1024)\n",
        "        print(f\"  File size: {file_size_mb:.2f} MB\")\n",
        "        \n",
        "        # Choose strategy\n",
        "        if file_size_mb < 10:\n",
        "            # Normal processing\n",
        "            print(\"  Strategy: Normal read\")\n",
        "            df = pd.read_excel(input_file)\n",
        "            \n",
        "            # Process (example)\n",
        "            df['Total'] = df['Sales'] * df['Quantity']\n",
        "            \n",
        "            # Save\n",
        "            df.to_excel(output_file, index=False)\n",
        "            \n",
        "        else:\n",
        "            # Chunked processing\n",
        "            print(f\"  Strategy: Chunked read (chunk size: {chunk_size})\")\n",
        "            \n",
        "            all_chunks = []\n",
        "            skip_rows = 0\n",
        "            chunk_num = 0\n",
        "            \n",
        "            while True:\n",
        "                if skip_rows > 0:\n",
        "                    df_chunk = pd.read_excel(\n",
        "                        input_file,\n",
        "                        skiprows=range(1, skip_rows + 1),\n",
        "                        nrows=chunk_size\n",
        "                    )\n",
        "                else:\n",
        "                    df_chunk = pd.read_excel(input_file, nrows=chunk_size)\n",
        "                \n",
        "                if df_chunk.empty:\n",
        "                    break\n",
        "                \n",
        "                chunk_num += 1\n",
        "                \n",
        "                # Process chunk\n",
        "                df_chunk['Total'] = df_chunk['Sales'] * df_chunk['Quantity']\n",
        "                all_chunks.append(df_chunk)\n",
        "                \n",
        "                print(f\"  Processed chunk {chunk_num}: {len(df_chunk):,} rows\")\n",
        "                \n",
        "                skip_rows += chunk_size\n",
        "                \n",
        "                if len(df_chunk) < chunk_size:\n",
        "                    break\n",
        "            \n",
        "            # Combine and save\n",
        "            df_final = pd.concat(all_chunks, ignore_index=True)\n",
        "            df_final.to_excel(output_file, index=False)\n",
        "            \n",
        "        print(f\"  âœ“ Success: Saved to {Path(output_file).name}\")\n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  âœ— Error: {e}\")\n",
        "        return False\n",
        "\n",
        "# Test production processor\n",
        "processed_file = output_dir / 'processed_large.xlsx'\n",
        "success = production_file_processor(large_file, processed_file)\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"SESSION 9 COMPLETE!\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "print(\"What you learned:\")\n",
        "print(\"âœ“ Measure and categorize file sizes\")\n",
        "print(\"âœ“ Read files in chunks\")\n",
        "print(\"âœ“ Use openpyxl streaming (read-only mode)\")\n",
        "print(\"âœ“ Write efficiently with write-only mode\")\n",
        "print(\"âœ“ Optimize data types for memory savings\")\n",
        "print(\"âœ“ Read only needed columns\")\n",
        "print(\"âœ“ Convert to CSV for faster processing\")\n",
        "print(\"âœ“ Process multiple files in batch\")\n",
        "print(\"âœ“ Build production-ready file processors\")\n",
        "print(\"âœ“ Handle large files without memory issues\")\n",
        "print()\n",
        "print(\"Files created:\")\n",
        "for file in sorted(output_dir.glob('*.xlsx')):\n",
        "    print(f\"  - {file.name}\")\n",
        "print()\n",
        "print(\"Performance Tips:\")\n",
        "print(\"  - Use chunking for files > 50MB\")\n",
        "print(\"  - Use streaming for files > 200MB\")\n",
        "print(\"  - Convert to CSV when possible\")\n",
        "print(\"  - Optimize data types\")\n",
        "print(\"  - Read only needed columns\")\n",
        "print()\n",
        "print(\"Next: Session 10 - Automation and Batch Processing\")\n",
        "print(\"=\" * 80)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}